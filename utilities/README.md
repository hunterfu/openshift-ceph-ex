## Openshift Storage Test Script

The [oc-test](oc-test) script can be used to verify/validate an OSE environment (specifying --verify), or both verify the OSE setup and run one or more storage plugin related test suites. All pods created use the busybox container image and the container's mount is always */usr/share/busybox*.

Here's is the full syntax, which is displayed when all script arguments are omitted:

```
  oc-test - execute one or more ose storage tests

SYNTAX
  oc-test [test-name] [options]

DESCRIPTION
  Verifies the ose-master environment and, optionally, executes one or more
  storage tests on the ose-master server.

  test-name  A list of one or more tests to run. Expected values are:
               general (default),
               nfs,
               gluster,
               rbd (or ceph),
               all.
             More than one test is specified with a comma separator, eg.
             "nfs,gluster".

  --verify   Only test the ose enviromment but don't run any tests.
  --master <node>
             Hostname or ip of the ose-master node, default is local host
  --oc-prefix <path>
             An optional path prefix appended to all "oc" commands, eg.
             "/root/origin/_output/local/bin/linux/amd64" for local ose builds.
  --sgid <number>
             An optional supplemental group ID to be applied to the pods created
             by various tests. If omitted the default is the first id in the
             supplemental group ID range defined for the current project. See:
               $ oc get ns <project> -o yaml  # for the various IDs
  --nfs-server <hostname|ip>
             NFS server's hostname or ip. Required if performing the nfs tests,
             otherwise ignored.
  --gluster-nodes <node-list>
             A list of 2 or more gluster storage node IP addresses (comma
             separator), which become the ose endpoints. Hostnames are converted
             to IPs using getent, but if DNS or /etc/hosts is not setup
             correctly it is better to supply IPs. Required if performing the
             gluster tests, otherwise ignored. Eg.:
               192.168.122.21,192.168.122.22
  --gluster-vol <volName>
             The name of an existing gluster/RHS volume. Required if performing
             the gluster tests, otherwise ignored.
  --rbd-monitors <node-list>
             A list of 1 or more ceph Monitor IP addresses (comma separated).
             Hostnames are converted to IPs using getent, but if DNS or
             /etc/hosts is not setup correctly it is better to supply the IP.
             If the default port of 6789 is not used then :portNum must follow
             all IPs not using the default port.  Required if performing the 
             ceph-rbd tests, otherwise ignored. Eg:
               192.168.122.133:6788,192.168.122.134
  --ceph-secret64 <base64 value>
             The base-64 encoded secret string generated by running:
               ceph auth get-key client.admin  , followed by pasting that output
             to the base64 command:
               echo -n "<output-from-ceph-auth-get-key>" | base64
             If omitted and the rbd test is requested and a provided ceph 
             monitor is reachable via ssh, then this value is calculated by the
             rbd test and is not required. It is ignore by all tests other than
             the ceph-rbd tests.
  --rbd-image <name>
             The name of the ceph-rbd image. If the rbd pool is not defaulted to
             "rbd" then the pool name is also required in the form of:
               <pool-name>/<image-name>
             Ignored for all tests other than the ceph-rbd tests.
  --version  Show the version string.
  -q         Suppress all prompts and reduce instructional output.
```

### Examples:
 1. To simply verify the target OSE environment, where "rhel7-ose-1" is the name of the OSE master host:
  ```
  ./oc-test --verify --master rhel7-ose-1
  
  *** Only validating the environment on ose-master "rhel7-ose-1"

*** Validating ose-master: "rhel7-ose-1"...

Login successful.

Using project "default".

You have access to the following projects and can switch between them with 'oc project <projectname>':

  * default (current)
  * openshift
  * openshift-infra

... validated
===================================
 Master node     : rhel7-ose-1
 Current project : default
   Sup User IDs  : 12345/10
   Sup Group IDs : 5555-5555,1000000000/10000
 Supplied Sup GID: <none>
 Pod's Group ID  : 5555
===================================
  ```

 2. To verify the target OSE environment and use a non-official version of origin (on the same master):
  ```
./oc-test --verify --master rhel7-ose-1 --oc-prefix /root/origin/_output/local/bin/linux/amd64
  ```

 3. To run the most basic test suite ("general" tests), which is the default if no tests are requested, using the same master host:
  ```
  ./oc-test --master rhel7-ose-1

*** Will run 1 test on ose-master "rhel7-ose-1":
       general

*** Validating ose-master: "rhel7-ose-1"...

Login successful.

Using project "default".

You have access to the following projects and can switch between them with 'oc project <projectname>':

  * default (current)
  * openshift
  * openshift-infra
... validated
...
*** Executing tests ...

*** General test suite ***"

    These baseline tests test that SGID  works in emptyDir and in
    hostPath. No PVs or claims are used here.

Press any key to continue...

----------
General Test 1: busybox, emptyDir, SGID 5555:
... deleting pod "general-pod1" (if it exists)...
pod "general-pod1" created
... checking pod "general-pod1" ...

----------
General Test 2: busybox, hostPath plugin, SGID 5555:
... deleting pod "general-pod2" (if it exists)...
pod "general-pod2" created
... checking pod "general-pod2" ...

***
*** Done with tests: 0 errors
***
  ```
  Use ``` -q ``` to suppress the "continue" prompt and reduce instructional output.
  
  
 4. To run the **NFS** test suite where the NFS server is "f21-nfs":
  ```
  ./oc-test --master rhel7-ose-1 --nfs f21-nfs  nfs

*** Will run 1 test on ose-master "rhel7-ose-1":
       nfs

*** Validating ose-master: "rhel7-ose-1"...

Login successful.

Using project "default".

You have access to the following projects and can switch between them with 'oc project <projectname>':
  * default (current)
  * openshift
  * openshift-infra
... validated

*** Executing tests ...

*** NFS test suite ***

Connecting to f21-nfs via ssh. You may need to enter a password.

root@f21-nfs's password: <nfs server password entered>

    Busybox is run with a volume mounted to the NFS server: f21-nfs.
    Remember to open port 2049 on the NFS server. A good test for this is,
    from the openshift-master, "rhel7-ose-1", execute:
       $ telnet f21-nfs 2049  # ctrl-c to exit
    To open port 2049 execute (on the NFS server):
       $ iptables -I INPUT 1 -p tcp --dport 2049 -j ACCEPTA

    Also, on the NFS server, edit /etc/exports to include /opt/nfs, eg:
       /opt/nfs *(rw,sync,no_root_squash)

    The group ID for the NFS export directory /opt/nfs is:
       5555
    and permissions on this directory are:
       drwxrwS---.

    To edit the range of supplemental group IDs, on the openshift-master, use:
       $ oc edit ns default
    and change the 'openshift.io/sa.scc.supplemental-groups' range to
    include the NFS group ID for /opt/nfs.  Also, use:
       $ oc get ns default -o yaml
    to see the values for various IDs in the "default" project.

    On the other hand, if it's ok to change the perms on /opt/nfs to match
    openshift's range of groups, then execute (on f21-nfs):
       $ chgrp 5555 /opt/nfs && chmod g+srw /opt/nfs

    Note: it may necessary to restart NFS:
       $ systemctl restart rpcbind nfs

Press any key to continue...

----------
NFS Test 1: baseline: busybox, nfs plugin:
... deleting pod "nfs-pod1" (if it exists)...
pod "nfs-pod1" created
... checking pod "nfs-pod1" ...
... checking mount type "nfs" for pod "nfs-pod1" ...

----------
NFS Test 2: busybox, nfs plugin, SGID 5555:
... deleting pod "nfs-pod2" (if it exists)...
pod "nfs-pod2" created
... checking pod "nfs-pod2" ...
... checking mount type "nfs" for pod "nfs-pod2" ...

----------
NFS Test 3: busybox, PV, PVC, SGID 5555:
... deleting pv "nfs-pv" (if it exists)...
persistentvolume "nfs-pv" created
... checking PV "nfs-pv" ...
... deleting pvc "nfs-pvc" (if it exists)...
persistentvolumeclaim "nfs-pvc" created
... checking PVC "nfs-pvc" ...
... deleting pod "nfs-pod3" (if it exists)...
pod "nfs-pod3" created
... checking pod "nfs-pod3" ...
... checking mount type "nfs" for pod "nfs-pod3" ...

***
*** Done with tests: 0 errors
***
  ```
  Again ``` -q ``` suppresses the continue prompt and the bulk of the nfs instructional output. Also ``` --sgid ``` can be supplied to set the Group ID in the busybox container, which is useful for various access/permissions issues.
  
  
 5. To run the **Gluster** storage test suite:
  ```
  ./oc-test --master rhel7-ose-1 --gluster-vol=HadoopVol --gluster-nodes=rhs-1.vm,rhs-2.vm gluster

*** Will run 1 test on ose-master "rhel7-ose-1":
       gluster

*** Validating ose-master: "rhel7-ose-1"...

Login successful.

Using project "default".

You have access to the following projects and can switch between them with 'oc project <projectname>':

  * default (current)
  * openshift
  * openshift-infra

... validated

===================================
 Master node     : rhel7-ose-1
 Current project : default
   Sup User IDs  : 12345/10
   Sup Group IDs : 5555-5555,1000000000/10000
 Supplied Sup GID: <none>
 Pod's Group ID  : 5555
===================================

*** Executing tests ...

*** Gluster test suite ***

    The supplied gluster storage nodes (endpoints) and the glusterfs plugin
    are tested using the busybox container to access the HadoopVol volume.
    On one of the gluster nodes, eg. 192.168.122.21, ensure that gluster
    is running, that the "HadoopVol" volume is active, and that the volume
    mount exists. Eg:
       $ gluster peer status
       $ gluster volume status HadoopVol
       $ mount | grep glusterfs
       # if HadoopVol is not displayed, then:
       $ mount -a # assuming the vol mount is present in /etc/fstab
       # if the vol mount is not in /etc/fstab, then add it, eg:
       192.168.122.21:/HadoopVol /mnt/glusterfs/HadoopVol glusterfs _netdev 0 0
       # and make sure the bricks are mounted as well...

    On the ose nodes make sure to install the gluster-client.

Press any key to continue...

----------
Gluster Test 1: baseline: busybox, glusterfs plugin:
... deleting endpoint "gluster-endpoints" (if it exists)...
endpoints "gluster-endpoints" created
... checking endpoint "gluster-endpoints" ...
... deleting pod "gluster-pod1" (if it exists)...
pod "gluster-pod1" created
... checking pod "gluster-pod1" ...
... checking mount type "glusterfs" for pod "gluster-pod1" ...

----------
Gluster Test 2: busybox, glusterfs plugin, SGID 5555:
... deleting pod "gluster-pod2" (if it exists)...
pod "gluster-pod2" created
... checking pod "gluster-pod2" ...
... checking mount type "glusterfs" for pod "gluster-pod2" ...

----------
Gluster Test 3: busybox, PV, PVC, SGID 5555:
... deleting pv "gluster-pv" (if it exists)...
persistentvolume "gluster-pv" created
... checking PV "gluster-pv" ...
... deleting pvc "gluster-pvc" (if it exists)...
persistentvolumeclaim "gluster-pvc" created
... checking PVC "gluster-pvc" ...
... deleting pod "gluster-pod3" (if it exists)...
pod "gluster-pod3" created
... checking pod "gluster-pod3" ...
... checking mount type "glusterfs" for pod "gluster-pod3" ...

***
*** Done with tests: 0 errors
***
  ```
 6. To run the **Ceph-RBD** test suite:
  ```
   ./oc-test --master rhel7-ose-1 --rbd-monitors 192.168.122.133 --rbd-image ceph-image rbd
*** Will run 1 test on ose-master "rhel7-ose-1":
       rbd

*** Validating ose-master: "rhel7-ose-1"...

Login successful.

Using project "default".

You have access to the following projects and can switch between them with 'oc project <projectname>':

  * default (current)
  * openshift
  * openshift-infra
  
... validated
===================================
 Master node     : rhel7-ose-1
 Current project : default
   Sup User IDs  : 12345/10
   Sup Group IDs : 5555-5555,1000000000/10000
 Supplied Sup GID: <none>
 Pod's Group ID  : 5555
===================================

*** Executing tests ...

*** RBD test suite ***

Calculating base64 ceph secret value...
Connecting to 192.168.122.133 via ssh. You may need to enter a password.

root@192.168.122.133's password: <entered password for ceph mon node>
Computed ceph secret: QVFBOFF2SlZheUJQRVJBQWgvS2cwT1laQUhPQno3akZwekxxdGc9PQ==

    Busybox is run with a volume mounted via the RBD plugin. The ceph user is
    currently hard-coded to "admin" and the file system is assumed to be ext4.
    (Both of these assumptions can easily be removed and added as script args.) 

    Ceph needs to be installed and running correctly. Here are a few tips to 
    perform on one of the monitors (eg. 192.168.122.133):
      rbd create ceph-image -s 1024  # create the image
      rbd map ceph-image             # map the image to the default pool, 'rbd'
      rbd showmapped
      ls /dev/rbd*                   # see which /dev/rbd device is used
      mkfs.ext4 /dev/rbd0            # create the file system
      # provide the below output as the --ceph-secret64 value
      ceph auth get-key client.admin | base64

      # on rhel7-ose-1:
      yum install -y ceph-common

Press any key to continue...

----------
RBD Test 1: baseline: busybox, ceph-rbd plugin:
... deleting secret "ceph-secret" (if it exists)...
secret "ceph-secret" created
... checking secret "ceph-secret" ...
... deleting pod "rbd-pod1" (if it exists)...
pod "rbd-pod1" created
... checking pod "rbd-pod1" ...
... checking mount type "rbd" for pod "rbd-pod1" ...

----------
RBD Test 2: busybox, ceph-rbd plugin, PV, PVC, SGID=5555:
... deleting pv "rbd-pv" (if it exists)...
persistentvolume "rbd-pv" created
... checking PV "rbd-pv" ...
... deleting pvc "rbd-pvc" (if it exists)...
persistentvolumeclaim "rbd-pvc" created
... checking PVC "rbd-pvc" ...
... deleting pod "rbd-pod2" (if it exists)...
pod "rbd-pod2" created
... checking pod "rbd-pod2" ...
... checking mount type "rbd" for pod "rbd-pod2" ...

***
*** Done with tests: 0 errors
***
  ```

  7. To run **ALL** of the test suites in one fell swoop, somewhat quietly:
   ```
   ./oc-test --master rhel7-ose-1 --gluster-vol=HadoopVol --gluster-nodes=rhs-1.vm,rhs-2.vm --nfs-server=f21-nfs  --rbd-monitors 192.168.122.133 --rbd-image ceph-image  all -q

*** Will run 4 tests on ose-master "rhel7-ose-1":
       general
       nfs
       gluster
       rbd

*** Validating ose-master: "rhel7-ose-1"...

Login successful.

Using project "default".

You have access to the following projects and can switch between them with 'oc project <projectname>':

  * default (current)
  * openshift
  * openshift-infra

... validated

*** Executing tests ...

----------
General Test 1: busybox, emptyDir, SGID 5555:
... deleting pod "general-pod1" (if it exists)...
pod "general-pod1" created
... checking pod "general-pod1" ...

----------
General Test 2: busybox, hostPath plugin, SGID 5555:
... deleting pod "general-pod2" (if it exists)...
pod "general-pod2" created
... checking pod "general-pod2" ...

----------
Gluster Test 1: baseline: busybox, glusterfs plugin:
... deleting endpoint "gluster-endpoints" (if it exists)...
endpoints "gluster-endpoints" created
... checking endpoint "gluster-endpoints" ...
... deleting pod "gluster-pod1" (if it exists)...
pod "gluster-pod1" created
... checking pod "gluster-pod1" ...
... checking mount type "glusterfs" for pod "gluster-pod1" ...

----------
Gluster Test 2: busybox, glusterfs plugin, SGID 5555:
... deleting pod "gluster-pod2" (if it exists)...
pod "gluster-pod2" created
... checking pod "gluster-pod2" ...
... checking mount type "glusterfs" for pod "gluster-pod2" ...

----------
Gluster Test 3: busybox, PV, PVC, SGID 5555:
... deleting pvc "gluster-pvc" (if it exists)...
... deleting pv "gluster-pv" (if it exists)...
persistentvolume "gluster-pv" created
... checking PV "gluster-pv" ...
persistentvolumeclaim "gluster-pvc" created
... checking PVC "gluster-pvc" ...
... deleting pod "gluster-pod3" (if it exists)...
pod "gluster-pod3" created
... checking pod "gluster-pod3" ...
... checking mount type "glusterfs" for pod "gluster-pod3" ...

*** NFS test suite ***

Connecting to f21-nfs via ssh. You may need to enter a password.

root@f21-nfs's password: 
Connection closed by 192.168.122.73
WARN: cannot retrieve permissions on /opt/nfs from NFS server
      "f21-nfs" via ssh. Be aware of potential permission mismatches
      between openshift supplemental groups and the NFS mount
      directory.



----------
NFS Test 1: baseline: busybox, nfs plugin:
... deleting pod "nfs-pod1" (if it exists)...
pod "nfs-pod1" created
... checking pod "nfs-pod1" ...
... checking mount type "nfs" for pod "nfs-pod1" ...

----------
NFS Test 2: busybox, nfs plugin, SGID 5555:
... deleting pod "nfs-pod2" (if it exists)...
pod "nfs-pod2" created
... checking pod "nfs-pod2" ...
... checking mount type "nfs" for pod "nfs-pod2" ...

----------
NFS Test 3: busybox, PV, PVC, SGID 5555:
... deleting pvc "nfs-pvc" (if it exists)...
... deleting pv "nfs-pv" (if it exists)...
persistentvolume "nfs-pv" created
... checking PV "nfs-pv" ...
persistentvolumeclaim "nfs-pvc" created
... checking PVC "nfs-pvc" ...
... deleting pod "nfs-pod3" (if it exists)...
pod "nfs-pod3" created
... checking pod "nfs-pod3" ...
... checking mount type "nfs" for pod "nfs-pod3" ...

*** RBD test suite ***

Calculating base64 ceph secret value...
Connecting to 192.168.122.133 via ssh. You may need to enter a password.

root@192.168.122.133's password: 
Computed ceph secret: QVFBOFF2SlZheUJQRVJBQWgvS2cwT1laQUhPQno3akZwekxxdGc9PQ==


----------
RBD Test 1: baseline: busybox, ceph-rbd plugin:
... deleting secret "ceph-secret" (if it exists)...
secret "ceph-secret" created
... checking secret "ceph-secret" ...
... deleting pod "rbd-pod1" (if it exists)...
pod "rbd-pod1" created
... checking pod "rbd-pod1" ...
... checking mount type "rbd" for pod "rbd-pod1" ...

----------
RBD Test 2: busybox, ceph-rbd plugin, PV, PVC, SGID=5555:
... deleting pvc "rbd-pvc" (if it exists)...
... deleting pv "rbd-pv" (if it exists)...
persistentvolume "rbd-pv" created
... checking PV "rbd-pv" ...
persistentvolumeclaim "rbd-pvc" created
... checking PVC "rbd-pvc" ...
... deleting pod "rbd-pod2" (if it exists)...
pod "rbd-pod2" created
... checking pod "rbd-pod2" ...
... checking mount type "rbd" for pod "rbd-pod2" ...

***
*** Done with tests: 0 errors
***
   ```
