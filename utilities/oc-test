#!/bin/bash
#
# oc-test creates ose pods on the ose-master node (defaults to localhost) and
# runs one or more of the specified storage tests.
#
# The storage specific test functions are named xxx_test(), eg. general_test()
# or nfs_test(). The general approach, in order to not need separate yaml files
# for each test's pod spec, is to supply the yaml as stdin to the:
# oc create -f command.  Eg:
#   cat <<END | oc create -f -
#     <yaml here...>
#   END
#
# The storage tests are loosley based on Scott Creeley's doc:
#   https://mojo.redhat.com/docs/DOC-1050225

# See usage() for syntax.
#

VERSION=0.1
VERIFY_ONLY=0 # false
MASTER="$HOSTNAME"
OC_PREFIX=''
# test vars
ALL_TESTS='general,nfs,gluster,rbd'
TESTS='general' # default
GENERAL_TEST=1 # true, default
GLUSTER_TEST=0 # false
NFS_TEST=0     # false
RBD_TEST=0     # false


## functions ##

function usage() {

  cat <<END
  oc-test - execute one or more ose storage tests

SYNTAX
  oc-test [test-name] [options]

DESCRIPTION
  Verifies the ose-master environment and, optionally, executes one or more
  storage tests on the ose-master server.

  test-name  A list of one or more tests to run. Expected values are:
               general (default),
               nfs,
               gluster,
               rbd (or ceph),
               all.
             More than one test is specified with a comma separator, eg.
             "nfs,gluster".

  --verify   Only test the ose enviromment but don't run any tests.
  --master <node>
             Hostname or ip of the ose-master node, default is local host
  --oc-prefix <path>
             An optional path prefix appended to all "oc" commands, eg.
             "/root/origin/_output/local/bin/linux/amd64" for local ose builds.
  --sgid <number>
             An optional supplemental group ID to be applied to the pods created
             by various tests. If omitted the default is the first id in the
             supplemental group ID range defined for the current project. See:
               $ oc get ns <project> -o yaml  # for the various IDs
  --nfs-server <hostname|ip>
             NFS server's hostname or ip. Required if performing the nfs tests,
             otherwise ignored.
  --gluster-nodes <node-list>
             A list of 2 or more gluster storage node IP addresses (comma
             separator), which become the ose endpoints. Hostnames are converted
             to IPs using getent, but if DNS or /etc/hosts is not setup
             correctly it is better to supply IPs. Required if performing the
             gluster tests, otherwise ignored.
  --gluster-vol <volName>
             The name of an existing gluster/RHS volume. Required if performing
             the gluster tests, otherwise ignored.
  --rbd-monitor <node>
             The ceph Monitor node IP address. A hostname is converted to an IP
             using getent, but if DNS or /etc/hosts is not setup correctly it
             is better to supply the IP. Required if performing the ceph-rbd
             tests, otherwise ignored.
  --ceph-secret64 <base64 value>
             The base-64 encoded secret string generated by running:
               ceph auth get-key client.admin  , followed by pasting that output
             to the base64 command:
               echo -n "<output-from-ceph-auth-get-key>" | base64
             If omitted and the rbd test is requested and the provided ceph 
             monitor is reachable via ssh, then this value is calculated by the
             rbd test and is not required. It is ignore by all tests other than
             the ceph-rbd tests.
  --rbd-image <name>
             The name of the ceph-rbd image. If the rbd pool is not defaulted to
             "rbd" then the pool name is also required in the form of:
               <pool-name>/<image-name>
             Ignored for all tests other than the ceph-rbd tests.
  --version  Show the version string.
  -q         Suppress all prompts and reduce instructional output.

END

}

# output the list of tests with commas replaced by spaces so it is suitable
# for use as an array. Note: ceph and rbd are synonyms for the same test and
# therefore ceph is changed to rbd. Returns 1 for errors.
# $1= a list of 1 or more tests, comma separated.
function parse_tests() {

  local tests="${1/ceph/rbd}"
  local test

  tests="${tests//,/ }"

  # validate known test names
  for test in $tests; do
     if [[ ! "$ALL_TESTS" =~ "$test" ]] ; then
       echo "ERROR: unknown test \"$test\""
       return -1
     fi
  done

  echo "$tests"
  return 0
}

# set globals to true for each test passed in
function set_test_flags() {

  local tests="$@"
  local test

  for test in $tests; do
     case "$test" in
        general)
          GENERAL_TEST=1 # true
        ;;
        gluster)
          GLUSTER_TEST=1 # true
        ;;
        nfs)
          NFS_TEST=1 # true
        ;;
        rbd|ceph)
          RBD_TEST=1 # true
        ;;
     esac

  done

  return 0
}

# returns 1 on errors, else 0.
function parse_cmd() {

  local errcnt=0
  local opts='q'
  local long_opts='version,master:,verify,oc-prefix:,nfs-server:,gluster-nodes:,gluster-vol:,sgid:,rbd-monitor:,ceph-secret64:,rbd-image:'

  (( $# == 0 )) && usage && exit -1

  eval set -- "$(getopt -o "$opts" --long $long_opts -- $@)"

  while true; do
      case "$1" in
        -q)
          QUIET=1 # true
          shift; continue
        ;;

        --version)
          echo $VERSION
          exit 0
        ;;

        --verify)
          VERIFY_ONLY=1; # true
          shift; continue
        ;;

        --master)
          MASTER="$2"
          shift 2; continue
        ;;

        --sgid)
          TGT_SGID="$2"
          shift 2; continue
        ;;

        --oc-prefix)
          OC_PREFIX="$2"
          shift 2; continue
        ;;

        --nfs-server)
          NFS="$2"
          shift 2; continue
        ;;

        --gluster-nodes)
          GLUSTER_NODES="$2" #comma separated list
          shift 2; continue
        ;;

        --gluster-vol)
          GLUSTER_VOL="$2"
          shift 2; continue
        ;;

        --rbd-monitor)
          MONITOR="$2"
          shift 2; continue
        ;;

        --ceph-secret64)
          CEPH_SECRET64="$2"
          shift 2; continue
        ;;

        --rbd-image)
          RBD_IMAGE="$2"
          shift 2; continue
        ;;

        --)
          shift; break
        ;;
      esac
  done

  if (( $# > 1 )) ; then
    shift
    echo "Syntax error: unexpected command line arg: $@"
    return 1
  fi

  if (( $# == 1 )) ; then # test arg is optional; default is general tests
    GENERAL_TEST=0 # false, don't assume general is in the list of tests
    TESTS="$1"; shift
    # handle "all" tests
    if [[ "$TESTS" == 'all' ]] ; then
      TESTS="$ALL_TESTS"
    fi
  fi

  # handle a list of tests
  TESTS="$(parse_tests "$TESTS")"
  if (( $? != 0 )) ; then
    echo "$TESTS" # error msg
    return -1
  fi
  TESTS=($TESTS)  # array
  NUM_TESTS=${#TESTS[@]}
  set_test_flags ${TESTS[@]}

  # --oc-prefix:
  if [[ -n "$OC_PREFIX" ]] ; then
    # oc prefix must end in a '/'
    [[ "$OC_PREFIX" != */ ]] && OC_PREFIX="$OC_PREFIX/"
    # make sure oc prefix exists as a dir on the master node
    if ! ssh $MASTER "[[ -d $OC_PREFIX ]]" ; then
      echo "ERROR: no directory named \"$OC_PREFIX\" on master node ($MASTER)"
      return 1
    fi
  fi

  # --nfs-server:
  if (( NFS_TEST )) ; then
    if [[ -z "$NFS" ]] ; then
      echo "ERROR: missing nfs server argument"
      echo
      usage && return 1
    fi
  fi

  # --gluster-nodes:
  if (( GLUSTER_TEST )) ; then
    if [[ -z "$GLUSTER_NODES" ]] ; then
      echo "ERROR: missing list of gluster nodes"
      echo
      usage && return 1
    fi
    if [[ -z "$GLUSTER_VOL" ]] ; then
      echo "ERROR: missing the gluster volume"
      echo
      usage && return 1
    fi
    GLUSTER_NODES=(${GLUSTER_NODES//,/ }) # array
    # convert hostnames to IPs, if possible
    GLUSTER_NODES=($(convert_host_to_ip ${GLUSTER_NODES[@]}))
    NUM_ENDPOINTS=${#GLUSTER_NODES[@]}
  fi

  # --rbd-monitor:
  if (( RBD_TEST )) ; then
    # convert hostname to IP, if possible
    MONITOR="$(convert_host_to_ip $MONITOR)"
  fi

  return 0
}

# returns true (0) if the passed-in id is within (inclusive) the pased-in range.
#   arg1=test id, arg2=range.
# Ranges are of the form: <startID>/<count>  or <startID>-<endID>  and
# a list of ranges are separated by a comma (no spaces).  Eg.
#   range-1,range-2,...range-N
# Note: the <startID>/<count> form is converted to a dash range by adding
#   count-1 to <startID>. Eg. 1000/10  => 1000-1009.
function id_in_range() {

  local test_id=$1; local range="${2//,/ }" # list
  local id; local r; local start; local end; local cnt

  for r in ${range[@]}; do
     if [[ $r == *"/"* ]] ; then # <start>/<count> form
       start=${r%/*}; cnt=${r#*/}; let end=start+cnt-1
     else # <start>-<end> form
       start=${r%-*}; end=${r#*-}
     fi
     # test if test_id is within this range
     (( start <= test_id && test_id <= end )) && return 0 # true
  done

  return 1 # false
}

# sets the SGIDs, SUIDs, OS_SGID, OS_SUID and the USE_SGID global variables to
# the values found in the project definition for the passed-in namespace. 
function get_supplemental_ids() {

  local ns="$1"
  local out; local ids

  out="$(ssh $MASTER "${OC_PREFIX}oc get ns $ns -o yaml")"

  # SGIDs:
  ids="$(grep 'openshift.io/sa.scc.supplemental-groups:' <<<"$out")"
  SGIDs="${ids#*: }"  # keep the starting id and range
  OS_SGID=${SGIDs%/*} # first number in range
  # use either the supplied --sgid= value or the openshift default
  USE_SGID=${TGT_SGID:-$OS_SGID}

  # error out if the target sgid is not w/in the openshift range
  if ! id_in_range $USE_SGID "$SGIDs" ; then
    echo "ERROR: target SGID of $USE_SGID is not within the range of supplemental"
    echo "       group IDs defined for the \"$NS\" project. Openshift's SGID"
    echo "       range is: $SGIDs. Either supply a different --sgid= value or"
    echo "       change the 'openshift.io/sa.scc.supplemental-groups' range"
    echo "       using the \"oc edit ns $NS\" command."
    return 1
  fi

  # SUIDs:
  ids="$(grep 'openshift.io/sa.scc.uid-range:' <<<"$out")"
  SUIDs="${ids#*: }"  # keep the starting id and range
  OS_SUID=${SUIDs%/*} # first number in range

  return 0
}

# convert the passed-in list of potential hostnames to ip addresses.
# output the list with space delimiters so it is easily made into an array.
function convert_host_to_ip() {

  local list=($*) # array
  local ips=''; local node; local ip

  # nested function: returns true (0) if the passed-in name matches an ip-addr.
  function is_ip_addr() {

    local octet='(25[0-5]|2[0-4][0-9]|[01]?[0-9]?[0-9])' # cannot exceed 255
    local ipv4="^$octet\.$octet\.$octet\.$octet$"

    [[ "$1" =~ $ipv4 ]] && return 0 # true
    return 1 # false
  }

  # main #

  for node in ${list[*]}; do
     if is_ip_addr $node ; then
       ips+="$node "
     else
       ip="$(getent hosts $node)" # uses dns or /etc/hosts
       if (( $? != 0 )) || [[ -z "$ip" ]] ; then
         ips+="$node "
       else
         ips+="${ip%% *} " # just ip-addr
       fi
     fi
  done

  echo "$ips"
  return 0
}

# echos the current namespace (=project) without any quotemarks.
function get_current_namespace() {

  local ns

  ns=($(ssh $MASTER "${OC_PREFIX}oc project")) # array
  ns=${ns[2]} # includes quotes
  ns=${ns%\"}; ns=${ns#\"} # remove enclosing quotes

  echo "$ns"
  return 0
}

# validate openshift env on passed-in node. Return 1 on error. If no 
# validation errors then collected ose state variables are echoed. 
function validate_ose_env() {

  local out

  echo "*** Validating ose-master: \"$MASTER\"..."
  echo

  ssh $MASTER "${OC_PREFIX}oc login -u admin -p ignored"
  (( $? != 0 )) && return 1
  
  # oc get nodes
  echo
  out="$(ssh $MASTER "${OC_PREFIX}oc get nodes")"
  if grep -q NotReady <<<"$out" ; then
    echo -e "ERROR: 1 or more nodes are not ready:\n$out"
    return 1
  fi

  # oc get scc
  echo
  out="$(ssh $MASTER "${OC_PREFIX}oc get scc")"
  if awk '{print $4}' <<<"$out" | grep -q false ; then #HOSTDIR col has a false
    echo "ERROR: HOSTDIR is false:"
    echo "$out"
    echo
    echo "Use the \"oc edit scc {privileged|restricted}\" command and set"
    echo "'allowHostDirVolumePlugin' to true."
    return 1
  fi

  echo "... validated"
  echo
  echo "==================================="
  echo " Master node     : $MASTER"
  echo " Current project : $NS"
  echo "   Sup User IDs  : $SUIDs"
  echo "   Sup Group IDs : $SGIDs"
  echo " Supplied Sup GID: ${TGT_SGID:-<none>}"
  echo " Pod's Group ID  : $USE_SGID"
  echo "==================================="
  echo

  return 0
}

# deletes the passed-in pod.
function delete_pod() {

  local pod=$1
  local i; local err; local out; local TRIES=10; local SLEEP=3

  # delete the target pod
  echo "... deleting pod \"$pod\" (if it exists)..."
  ssh -q $MASTER "${OC_PREFIX}oc delete pod $pod >&/dev/null"

  # wait for the delete to complete...
  for ((i=0; i<$TRIES; i++)); do
     sleep $SLEEP
     out="$(ssh $MASTER "${OC_PREFIX}oc get pod $pod 2>&1")"
     err=$?
     if (( err != 0 )) && 
        grep -q " not found" <<<"$out" ; then # expected err
       return 0
     fi
     if (( err != 0 )) ; then # unexpected
       echo "$out"
       return 1
     fi
  done

  echo -e "ERROR: $TRIES tries attempted to delete pod \"$pod\":\n$out"
  return 1
}

# deletes the passed-in pvc (claim).
function delete_pvc() {

  local pvc=$1
  local i; local err; local out; local TRIES=3; local SLEEP=3

  # delete the target pvc
  echo "... deleting pvc \"$pvc\" (if it exists)..."
  ssh -q $MASTER "${OC_PREFIX}oc delete pvc $pvc >&/dev/null"

  # wait for the delete to complete...
  for ((i=0; i<$TRIES; i++)); do
     sleep $SLEEP
     out="$(ssh $MASTER "${OC_PREFIX}oc get pvc $pvc 2>&1")"
     err=$?
     if (( err != 0 )) && 
        grep -q " not found" <<<"$out" ; then # expected err
       return 0
     fi
     if (( err != 0 )) ; then # unexpected
       echo "$out"
       return 1
     fi
  done

  echo -e "ERROR: $TRIES tries attempted to delete pvc \"$pvc\":\n$out"
  return 1
}

# deletes the passed-in pv (Persistent Volume).
function delete_pv() {

  local pv=$1
  local i; local err; local out; local TRIES=15; local SLEEP=2

  # delete the target pv
  echo "... deleting pv \"$pv\" (if it exists)..."
  ssh -q $MASTER "${OC_PREFIX}oc delete pv $pv >&/dev/null"

  # wait for the delete to complete...
  for ((i=0; i<$TRIES; i++)); do
     sleep $SLEEP
     out="$(ssh $MASTER "${OC_PREFIX}oc get pv $pv 2>&1")"
     err=$?
     if (( err != 0 )) && 
        grep -q " not found" <<<"$out" ; then # expected err
       return 0
     fi
     if (( err != 0 )) ; then # unexpected
       echo "$out"
       return 1
     fi
  done

  echo -e "ERROR: $TRIES tries attempted to delete pv \"$pv\":\n$out"
  return 1
}

# deletes the passed-in endpoint.
function delete_endpoint() {

  local endpt="$1"
  local i; local err; local out; local TRIES=3; local SLEEP=2

  # delete the target endpoint
  echo "... deleting endpoint \"$endpt\" (if it exists)..."
  ssh -q $MASTER "${OC_PREFIX}oc delete endpoints $endpt >&/dev/null"

  # wait for the delete to complete...
  for ((i=0; i<$TRIES; i++)); do
     sleep $SLEEP
     out="$(ssh $MASTER "${OC_PREFIX}oc get endpoints $endpt 2>&1")"
     err=$?
     if (( err != 0 )) && 
        grep -q " not found" <<<"$out" ; then # expected err
       return 0
     fi
     if (( err != 0 )) ; then # unexpected
       echo "$out"
       return 1
     fi
  done

  echo -e "ERROR: $TRIES tries attempted to delete endpoint \"$endpt\":\n$out"
  return 1
}

# deletes the passed-in secret.
function delete_secret() {

  local secret="$1"
  local i; local err; local out; local TRIES=3; local SLEEP=2

  # delete the target secret
  echo "... deleting secret \"$secret\" (if it exists)..."
  ssh -q $MASTER "${OC_PREFIX}oc delete secret $secret >&/dev/null"

  # wait for the delete to complete...
  for ((i=0; i<$TRIES; i++)); do
     sleep $SLEEP
     out="$(ssh $MASTER "${OC_PREFIX}oc get secret $secret 2>&1")"
     err=$?
     if (( err != 0 )) && 
        grep -q " not found" <<<"$out" ; then # expected err
       return 0
     fi
     if (( err != 0 )) ; then # unexpected
       echo "$out"
       return 1
     fi
  done

  echo -e "ERROR: $TRIES tries attempted to delete secret \"$secret\":\n$out"
  return 1
}

# returns 0 if the passed-in pod is running. Echos "oc get pod" output for all
# errors.
# $1= pod-name
function verify_new_pod() {

  local pod="$1"
  local i; local MAX=15 # num tries
  local SLEEP=3; local out

  echo "... checking pod \"$pod\" ..."

  for ((i=0; i<$MAX; i++)); do
     sleep $SLEEP
     out="$(ssh $MASTER "${OC_PREFIX}oc get pod $pod 2>&1")"
     (( $? != 0 )) && {
       echo "$out"; return 1; }
     if awk '{print $2}' <<<"$out" | grep -q '1/1' ; then # READY column
       return 0 # success
     fi
  done

  echo -e "ERROR: $(((MAX*SLEEP))) seconds waiting for pod \"$pod\" to start:\n$out"
  return 1
}

# returns 0 if the passed-in PV is running.
function verify_new_pv() {

  local pv="$1"
  local i; local TRIES=20; local SLEEP=3; local out

  echo "... checking PV \"$pv\" ..."

  for ((i=0; i<$TRIES; i++)); do
     sleep $SLEEP
     out="$(ssh $MASTER "${OC_PREFIX}oc get pv $pv 2>&1")"
     (( $? != 0 )) && { 
       echo "$out"; return 1; }
     if awk '{print $5}' <<<"$out" | grep -q 'Available' ; then # STATUS column
       return 0 # success
     fi
  done

  echo -e "ERROR: $(((TRIES*SLEEP))) seconds waiting for PV \"$pv\" to start:\n$out"
  return 1
}

# returns 0 if the passed-in PVC is running.
function verify_new_pvc() {

  local pvc="$1"
  local i; local TRIES=3; local SLEEP=3; local out

  echo "... checking PVC \"$pvc\" ..."

  for ((i=0; i<$TRIES; i++)); do
     sleep $SLEEP
     out="$(ssh $MASTER "${OC_PREFIX}oc get pvc $pvc 2>&1")"
     (( $? != 0 )) && { 
       echo "$out"; return 1; }
     if awk '{print $3}' <<<"$out" | grep -q 'Bound' ; then # STATUS column
       return 0 # success
     fi
  done

  echo -e "ERROR: $(((TRIES*SLEEP))) seconds waiting for PVC \"$pvc\" to start:\n$out"
  return 1
}

# returns 0 if the passed-in endpoint is running.
function verify_new_endpoint() {

  local endpt="$1"
  local i; local TRIES=3; local SLEEP=3; local out

  echo "... checking endpoint \"$endpt\" ..."

  for ((i=0; i<$TRIES; i++)); do
     sleep $SLEEP
     out="$(ssh $MASTER "${OC_PREFIX}oc get endpoints $endpt 2>&1")"
     (( $? != 0 )) && { 
       echo "$out"; return 1; }
     grep -q "$endpt" <<<"$out" && return 0
  done

  echo -e "ERROR: $(((TRIES*SLEEP))) seconds waiting for endpoints \"$endpt\" to start:\n$out"
  return 1
}

# returns 0 if the passed-in secret is running.
function verify_new_secret() {

  local secret="$1"
  local i; local TRIES=3; local SLEEP=2; local out

  echo "... checking secret \"$secret\" ..."

  for ((i=0; i<$TRIES; i++)); do
     sleep $SLEEP
     out="$(ssh $MASTER "${OC_PREFIX}oc get secret $secret 2>&1")"
     (( $? != 0 )) && { 
       echo "$out"; return 1; }
     grep -q 'Opaque' <<<"$out" && return 0
  done

  echo -e "ERROR: $(((TRIES*SLEEP))) seconds waiting for secret \"$secret\" to start:\n$out"
  return 1
}

# run the "general" tests.
function general_test() {

  local pod

  if (( ! QUIET )) ; then
    cat <<END_GENERAL
*** General test suite ***"

    These baseline tests test that SGID $SUSE_GID works in emptyDir and in
    hostPath. No PVs or claims are used here.

END_GENERAL

    read -p "Press any key to continue..." -t120 out
  fi

  echo
  echo "----------"
  echo "General Test 1: busybox, emptyDir, SGID $USE_SGID:"

  pod='general-pod1'
  delete_pod $pod

  cat <<END | ssh $MASTER "${OC_PREFIX}oc create -f -"
apiVersion: v1
kind: Pod
metadata:
  name: $pod
  labels:
    name: $pod
spec:
  containers:
  - name: $pod
    image: busybox
    command: ["sleep", "60000"]
  securityContext:
    supplementalGroups: [$USE_SGID]
    privileged: false
END

  verify_new_pod $pod || {
    echo "ERROR: general test #1"; return 1; }

  echo
  echo "----------"
  echo "General Test 2: busybox, hostPath plugin, SGID $USE_SGID:"

  pod='general-pod2'
  delete_pod $pod

  cat <<END | ssh $MASTER "${OC_PREFIX}oc create -f -"
apiVersion: v1
kind: Pod
metadata:
  name: $pod
  labels:
    name: $pod
spec:
  containers:
  - name: $pod
    image: busybox
    command: ["sleep", "60000"]
    volumeMounts:
    - name: bb-vol
      mountPath: /usr/share/busybox
  securityContext:
    supplementalGroups: [$USE_SGID]
    privileged: false
  volumes:
  - name: bb-vol
    hostPath:
      path: /opt/data
END

  verify_new_pod $pod || {
    echo "ERROR: general test #2"; return 1; }

  return 0
}

# run the "nfs" tests.
# Note: the user-specified --sgid is used in these tests. If omitted then the
#  default openshift sgid is used.
function nfs_test() {

  local pod; local pv; local pvc; local out
  local vol_mount='/opt/nfs'; local vol_perms; local vol_gid
  local sup_gids=$USE_SGID # may have the nfs vol's gid appended

  echo "*** NFS test suite ***"
  echo

  # get nfs server vol mount perms
  echo "Connecting to $NFS via ssh. You may need to enter a password."
  sleep 1
  echo

  out="$(ssh $NFS "ls -ld $vol_mount 2>&1")"
  if (( $? == 0 )) ; then
    out=($out) # array
    vol_perms="${out[0]}"
    vol_gid="${out[3]}"
    # append vol's sgid to pod's sgid?
    if id_in_range $vol_gid "$SGIDs" ; then
      if (( vol_gid != USE_SGID )) ; then # append vol gid to supp gids
        sup_gids+=",$vol_gid"     
        echo "Pod's supplemental groups are: $sup_gids"
      fi
    else
      echo "WARN: the NFS volume ${vol_mount}'s GID is NOT within the"
      echo "      \"$NS\" project's range of group IDs: $SGIDs, and"
      echo "      cannot be added to the pod's list of group IDs."
    fi

  else
    echo "WARN: cannot retrieve permissions on $vol_mount from NFS server"
    echo "      \"$NFS\" via ssh. Be aware of potential permission mismatches"
    echo "      between openshift supplemental groups and the NFS mount"
    echo "      directory."
    echo
    echo "$out"
  fi

  # show more info for running these tests...
  if (( ! QUIET )) ; then
    cat <<END_NFS

    Busybox is run with a volume mounted to the NFS server: $NFS.
    Remember to open port 2049 on the NFS server. A good test for this is,
    from the openshift-master, "$MASTER", execute:
       $ telnet $NFS 2049  # ctrl-c to exit
    To open port 2049 execute (on the NFS server):
       $ iptables -I INPUT 1 -p tcp --dport 2049 -j ACCEPTA

    Also, on the NFS server, edit /etc/exports to include /opt/nfs, eg:
       /opt/nfs *(rw,sync,no_root_squash)

    The group ID for the NFS export directory $vol_mount is:
       ${vol_gid:-<unavailable>}
    and permissions on this directory are:
       ${vol_perms:-<unavailable>}

    To edit the range of supplemental group IDs, on the openshift-master, use:
       $ oc edit ns $NS
    and change the 'openshift.io/sa.scc.supplemental-groups' range to
    include the NFS group ID for $vol_mount.  Also, use:
       $ oc get ns $NS -o yaml
    to see the values for various IDs in the "$NS" project.

    On the other hand, if it's ok to change the perms on $vol_mount to match
    openshift's range of groups, then execute (on $NFS):
       $ chgrp $OS_SGID /opt/nfs && chmod g+srw /opt/nfs

    Note: it may necessary to restart NFS:
       $ systemctl restart rpcbind nfs

END_NFS

    read -p "Press any key to continue..." -t120 out
  fi

  echo
  echo "----------"
  echo "NFS Test 1: baseline: busybox, nfs plugin:"

  pod='nfs-pod1'
  delete_pod $pod

  cat <<END | ssh $MASTER "${OC_PREFIX}oc create -f -"
apiVersion: v1
kind: Pod
metadata:
  name: $pod
  labels:
    name: $pod
spec:
  containers:
  - name: $pod
    image: busybox
    command: ["sleep", "60000"]
    volumeMounts:
    - name: nfs-vol1
      mountPath: /usr/share/busybox
  securityContext:
      privileged: false
  volumes:
  - name: nfs-vol1
    nfs:
      path: "$vol_mount"
      server: $NFS
      readOnly: false
END

  if ! verify_new_pod $pod ; then
    echo "ERROR: nfs test #1"
    echo "       Try restarting rpcbind and nfs on the nfs server \"$NFS\"."
    echo "       Also make sure that port 2049 on the NFS server is open, eg:"
    echo "         iptables -I INPUT 1 -p tcp --dport 2049 -j ACCEPT"
    return 1
  fi

  echo
  echo "----------"
  echo "NFS Test 2: busybox, nfs plugin, SGID $sup_gids:"

  pod='nfs-pod2'
  delete_pod $pod

  cat <<END | ssh $MASTER "${OC_PREFIX}oc create -f -"
apiVersion: v1
kind: Pod
metadata:
  name: $pod
  labels:
    name: $pod
spec:
  containers:
  - name: $pod
    image: busybox
    command: ["sleep", "60000"]
    volumeMounts:
    - name: nfs-vol2
      mountPath: /usr/share/busybox
  securityContext:
    supplementalGroups: [$sup_gids]
    privileged: false
  volumes:
  - name: nfs-vol2
    nfs:
      path: "$vol_mount"
      server: $NFS
      readOnly: false
END

  verify_new_pod $pod || {
    echo "ERROR: nfs test #2"; return 1; }

  echo
  echo "----------"
  echo "NFS Test 3: busybox, PV, PVC, SGID $sup_gids:"

  # a. create PV
  pv='nfs-pv'
  delete_pv $pv

  cat <<END | ssh $MASTER "${OC_PREFIX}oc create -f -"
apiVersion: v1
kind: PersistentVolume
metadata:
  name: $pv
spec:
  capacity:
    storage: 1Gi
  accessModes:
  - ReadWriteOnce
  nfs:
    path: $vol_mount
    server: $NFS
END

  verify_new_pv $pv || {
    echo "ERROR: nfs test #3: can't create PV"; return 1; }

  # b. create PVC
  pvc='nfs-pvc'
  delete_pvc $pvc

  cat <<END | ssh $MASTER "${OC_PREFIX}oc create -f -"
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: $pvc
spec:
  accessModes:
  - ReadWriteOnce
  resources:
     requests:
       storage: 1Gi
END

  verify_new_pvc $pvc || {
    echo "ERROR: nfs test #3: can't create PVC"; return 1; }

  # c. create pod using claim
  pod='nfs-pod3'
  delete_pod $pod

  cat <<END | ssh $MASTER "${OC_PREFIX}oc create -f -"
apiVersion: v1
kind: Pod
metadata:
  name: $pod
  labels:
    name: $pod
spec:
  containers:
  - name: $pod
    image: busybox
    command: ["sleep", "60000"]
    volumeMounts:
    - name: nfs-vol3
      mountPath: /usr/share/busybox
  securityContext:
    supplementalGroups: [$sup_gids]
    privileged: false
  volumes:
  - name: nfs-vol3
    persistentVolumeClaim:
      claimName: $pvc
END

  verify_new_pod $pod || {
    echo "ERROR: nfs test #3"; return 1; }

  return 0
}

# run the "glusterfs" tests.
function gluster_test() {

  local pod; local node; local endpoints=''; local dmy
  local endpt='gluster-endpoints'; local pv; local pvc

  if (( ! QUIET )) ; then
    cat <<END_GLUSTER
*** Gluster test suite ***

    The supplied gluster storage nodes (endpoints) and the glusterfs plugin
    are tested using the busybox container to access the $GLUSTER_VOL volume.
    On one of the gluster nodes, eg. ${GLUSTER_NODES[0]}, ensure that gluster
    is running, that the "$GLUSTER_VOL" volume is active, and that the volume
    mount exists. Eg:
       $ gluster peer status
       $ gluster volume status $GLUSTER_VOL
       $ mount | grep glusterfs
       # if $GLUSTER_VOL is not displayed, then:
       $ mount -a # assuming the vol mount is present in /etc/fstab
       # if the vol mount is not in /etc/fstab, then add it, eg:
       ${GLUSTER_NODES[0]}:/$GLUSTER_VOL /mnt/glusterfs/$GLUSTER_VOL glusterfs _netdev 0 0
       # and make sure the brick are mounted as well...

    On the ose nodes make sure to install the gluster-client.

END_GLUSTER
  
    read -p "Press any key to continue..." -t120 dmy
  fi

  echo
  echo "----------"
  echo "Gluster Test 1: baseline: busybox, glusterfs plugin:"

  # a. create endpoints
  delete_endpoint $endpt

  for node in ${GLUSTER_NODES[@]}; do # need newlines
     endpoints+="- addresses:"$'\n'
     endpoints+="  - ip: $node"$'\n'
     endpoints+="  ports:"$'\n'
     endpoints+="  - port: 1"$'\n' # ignored
     endpoints+="    protocol: TCP"$'\n'
  done

  cat <<END | ssh $MASTER "${OC_PREFIX}oc create -f -"
apiVersion: v1
kind: Endpoints
apiVersion: v1
metadata:
  name: $endpt
subsets:
$endpoints
END

  verify_new_endpoint $endpt || {
    echo "ERROR: gluster test #1: can't create endpoints"; return 1; }

  # b. create pod
  pod='gluster-pod1'
  delete_pod $pod

  cat <<END | ssh $MASTER "${OC_PREFIX}oc create -f -"
apiVersion: v1
kind: Pod
metadata:
  name: $pod
  labels:
    name: $pod
spec:
  containers:
  - name: $pod
    image: busybox
    command: ["sleep", "60000"]
    volumeMounts:
    - name: gluster-vol1
      mountPath: /usr/share/busybox
  volumes:
  - name: gluster-vol1
    glusterfs:
      endpoints: $endpt
      path: $GLUSTER_VOL
      readOnly: false
END

  verify_new_pod $pod || {
    echo "ERROR: gluster test #1"; return 1; }

  echo
  echo "----------"
  echo "Gluster Test 2: busybox, glusterfs plugin, SGID $USE_SGID:"

  pod='gluster-pod2'
  delete_pod $pod
  cat <<END | ssh $MASTER "${OC_PREFIX}oc create -f -"
apiVersion: v1
kind: Pod
metadata:
  name: $pod
  labels:
    name: $pod
spec:
  containers:
  - name: $pod
    image: busybox
    command: ["sleep", "60000"]
    volumeMounts:
    - name: gluster-vol2
      mountPath: /usr/share/busybox
  securityContext:
    supplementalGroups: [$USE_SGID]
    privileged: false
  volumes:
  - name: gluster-vol2
    glusterfs:
      endpoints: $endpt
      path: $GLUSTER_VOL
      readOnly: false
END

  verify_new_pod $pod || {
    echo "ERROR: gluster test #1"; return 1; }

  echo
  echo "----------"
  echo "Gluster Test 3: busybox, PV, PVC, SGID $USE_SGID:"

  # a. create PV
  pv='gluster-pv'
  delete_pv $pv

  cat <<END | ssh $MASTER "${OC_PREFIX}oc create -f -"
apiVersion: v1
kind: PersistentVolume
metadata:
  name: $pv
spec:
  capacity:
    storage: 1Gi
  accessModes:
  - ReadWriteOnce
  glusterfs:
    endpoints: $endpt
    path: $GLUSTER_VOL
    readOnly: false
END

  verify_new_pv $pv || {
    echo "ERROR: gluster test #2: can't create PV"; return 1; }

  # b. create PVC
  pvc='gluster-pvc'
  delete_pvc $pvc

  cat <<END | ssh $MASTER "${OC_PREFIX}oc create -f -"
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: $pvc
spec:
  accessModes:
  - ReadWriteOnce
  resources:
     requests:
       storage: 1Gi
END

  verify_new_pvc $pvc || {
    echo "ERROR: gluster test #2: can't create PVC"; return 1; }

  # c. create pod using claim
  pod='gluster-pod3'
  delete_pod $pod

  cat <<END | ssh $MASTER "${OC_PREFIX}oc create -f -"
apiVersion: v1
kind: Pod
metadata:
  name: $pod
  labels:
    name: $pod
spec:
  containers:
  - name: $pod
    image: busybox
    command: ["sleep", "60000"]
    volumeMounts:
    - name: gluster-vol3
      mountPath: /usr/share/busybox
  securityContext:
    supplementalGroups: [$USE_SGID]
    privileged: false
  volumes:
  - name: gluster-vol3
    persistentVolumeClaim:
      claimName: $pvc
END

  verify_new_pod $pod || {
    echo "ERROR: gluster test #3"; return 1; }

  return 0
}

# run the "ceph-rbd" tests.
function rbd_test() {

  local pod; local pv; local pvc; local pool; local image; local dmy
  local secret_name='ceph-secret'

  pool="${RBD_IMAGE%/*}" # pool name is optional
  [[ "$pool" == "$RBD_IMAGE" ]] && pool='rbd' # not specifie, use default
  RBD_IMAGE="${RBD_IMAGE#*/}" # remove pool name if present

  if (( ! QUIET )) ; then
    cat <<END_RBD
*** Ceph-RBD test suite ***

    The...

- ceph user is hard-coded to be admin
- filesytem is hard-coded to be ext4
- rbd create ceph-image -s 1024
- rbd map ceph-image
- rbd showmapped
- ls /dev/rbd*
- mkfs.ext4 /dev/rbd0
- ceph auth get-key client.admin
- echo -n "AQA8QvJVayBPERAAh/Kg0OYZAHOBz7jFpzLqtg==" | base64

END_RBD
  
    read -p "Press any key to continue..." -t120 dmy
  fi

  echo
  echo "----------"
  echo "RBD Test 1: baseline: busybox, ceph-rbd plugin:"

  # a. create ceph-secret
  delete_secret $secret_name

  # if the ceph-secret was not provided and we have ssh access to the monitor
  # then calculate the ceph-secret here
  if [[ -z "$CEPH_SECRET64" ]] ; then
CEPH_SECRET64="AQA8QvJVayBPERAAh/Kg0OYZAHOBz7jFpzLqtg==" ##TEMP!!!!
    CEPH_SECRET64="$(echo -n "$CEPH_SECRET64" | base64)"
echo "***CEPH_SECRET64=$CEPH_SECRET64"
  fi

  cat <<END | ssh $MASTER "${OC_PREFIX}oc create -f -"
apiVersion: v1
kind: Secret
metadata:
  name: $secret_name
data:
  key: $CEPH_SECRET64
END

  verify_new_secret $secret_name || {
    echo "ERROR: RBD test #1"; return 1; }

  # b. create the pod
  pod='rbd-pod1'
  delete_pod $pod

echo "***secret_name=$secret_name, pool=$pool, RBD_IMAGE=$RBD_IMAGE, MONITOR=$MONITOR"
  cat <<END | ssh $MASTER "${OC_PREFIX}oc create -f -"
apiVersion: v1
kind: Pod
metadata:
  name: $pod
  labels:
    name: $pod
spec:
  containers:
  - name: $pod
    image: busybox
    command: ["sleep", "60000"]
    volumeMounts:
    - name: rbd-vol1
      mountPath: /usr/share/busybox
  volumes:
  - name: rbd-vol1
    rbd: 
      monitors: 
      - $MONITOR
      pool: $pool
      image: $RBD_IMAGE
      user: admin
      secretRef: 
        name: $secret_name
      fsType: ext4
      readOnly: false
END

  verify_new_pod $pod || {
    echo "ERROR: RBD test #1"; return 1; }

  return 0
}

# execute the requested test(s).
# $@= list of 1 or more tests (space separator)
function execute_tests() {

  local tests="$@"
  local test; local errcnt=0

  echo "*** Executing tests ..."
  echo

  # execute each requested test:
  (( GENERAL_TEST )) && {
    general_test || ((errcnt++)); }

  (( GLUSTER_TEST )) && {
    gluster_test || ((errcnt++)); }

  (( NFS_TEST )) && {
    nfs_test || ((errcnt++)); }

  (( RBD_TEST )) && {
    rbd_test || ((errcnt++)); }

  echo
  echo "... done with tests: $errcnt errors"
  echo

  (( errcnt > 0 )) && return 1
  return 0
}


## main ##

parse_cmd $@ || exit -1

NS="$(get_current_namespace)"

# set supplemental group and user IDs as globals
get_supplemental_ids $NS || exit 1

echo
if (( VERIFY_ONLY )) ; then
  echo "*** Only validating the environment on ose-master \"$MASTER\""
else
  (( NUM_TESTS > 1 )) && plural='s' || plural=''
  echo "*** Will run $NUM_TESTS test$plural on ose-master \"$MASTER\":"
  for test in ${TESTS[@]}; do
     echo "       $test"
  done
fi
echo
sleep 2

validate_ose_env || exit 1

(( ! VERIFY_ONLY )) && execute_tests ${TESTS[@]} || exit 1

exit 0
