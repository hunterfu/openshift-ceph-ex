#!/bin/bash
#
# oc-test creates ose pods on the ose-master node (defaults to localhost) and
# runs one or more of the specified storage tests.
#
# See usage() for syntax.
#

VERSION=0.1
VERIFY_ONLY=0 # false
ALL_TESTS='general,nfs,gluster,rbd'
TESTS='general' # default
MASTER="$HOSTNAME"
OC_PREFIX=''


## functions ##

function usage() {

  cat <<END
  oc-test - execute one or more ose storage tests

SYNTAX
  oc-test [test-name] [options]

DESCRIPTION
  Verifies the ose-master environment and, optionally, executes one or more
  storage tests from the ose-master node.

  test-name  A list of one or more tests to run. Expected values are:
               general (default),
               nfs,
               gluster,
               rbd (or ceph),
               all.
             More than one test is specified with a comma separator, eg.
             "nfs,gluster".

  --verify   Only test the ose enviromment but don't run any tests.
  --master <node>
             Hostname or ip of the ose-master node, default is local host
  --oc-prefix [path]
             An optional path prefix appended to all "oc" commands, eg.
             "/root/origin/_output/local/bin/linux/amd64" for local ose builds.
  --nfs-server <hostname|ip>
             NFS server's hostname or ip. Required if performing the nfs tests,
             otherwise ignored.
  --gluster-nodes <list-of-nodes-comma-separated>
             A list of 2 or more gluster storage node IP addresses, which
             become the ose endpoints. Hostname are converted to IPs using
             getent, but if DNS or /etc/hosts is not setup correctly it is
             better to supply IPs. Required if performing the gluster tests,
             otherwise ignored.
  --gluster-vol <volName>
             The name of an existing gluster/RHS volume. Required if performing
             the gluster tests, otherwise ignored.
  --version  Show the version string.

END

}

# output the list of tests with commas replaced by spaces so it is suitable
# for use as an array. Note: ceph and rbd are synonyms for the same test and
# therefore ceph is changed to rbd. Returns 1 for errors.
# $1= a list of 1 or more tests, comma separated.
function parse_tests() {

  local tests="${1/ceph/rbd}"
  local test

  tests="${tests//,/ }"

  # validate known test names
  for test in $tests; do
     if [[ ! "$ALL_TESTS" =~ "$test" ]] ; then
       echo "ERROR: unknown test \"$test\""
       return -1
     fi
  done

  echo "$tests"
  return 0
}

# returns 1 on errors, else 0.
function parse_cmd() {

  local errcnt=0
  local long_opts='version,master:,verify,oc-prefix:,nfs-server:,gluster-nodes:,gluster-vol:'

  (( $# == 0 )) && usage && exit -1

  eval set -- "$(getopt -o 'y' --long $long_opts -- $@)"

  while true; do
      case "$1" in
        --version)
          echo $VERSION
          exit 0
        ;;

        --verify)
          VERIFY_ONLY=1; # true
          shift; continue
        ;;

        --master)
          MASTER="$2"
          shift 2; continue
        ;;

        --oc-prefix)
          OC_PREFIX="$2"
          shift 2; continue
        ;;

        --nfs-server)
          NFS="$2"
          shift 2; continue
        ;;

        --gluster-nodes)
          GLUSTER_NODES="$2" #comma separated list
          shift 2; continue
        ;;

        --gluster-vol)
          GLUSTER_VOL="$2"
          shift 2; continue
        ;;

        --)
          shift; break
        ;;
      esac
  done

  if (( $# > 1 )) ; then
    shift
    echo "Syntax error: unexpected command line arg: $@"
    return 1
  fi

  if (( $# == 1 )) ; then # test arg is optional
    TESTS="$1"; shift
    # handle "all" tests
    if [[ "$TESTS" == 'all' ]] ; then
      TESTS="$ALL_TESTS"
    fi
  fi

  # handle a list of tests
  TESTS="$(parse_tests "$TESTS")"
  if (( $? != 0 )) ; then
    echo "$TESTS" # error msg
    return -1
  fi
  TESTS=($TESTS)  # array
  NUM_TESTS=${#TESTS[@]}

  # --oc-prefix:
  if [[ -n "$OC_PREFIX" ]] ; then
    # oc prefix must end in a '/'
    [[ "$OC_PREFIX" != */ ]] && OC_PREFIX="$OC_PREFIX/"
    # make sure oc prefix exists as a dir on the master node
    if ! ssh $MASTER "[[ -d $OC_PREFIX ]]" ; then
      echo "ERROR: no directory named \"$OC_PREFIX\" on master node ($MASTER)"
      return 1
    fi
  fi

  # --nfs-server:
  if [[ "${TESTS[@]}" =~ nfs ]] ; then # nfs test requested
    if [[ -z "$NFS" ]] ; then
      echo "ERROR: missing nfs server argument"
      echo
      usage && return 1
    fi
  fi

  # --gluster-nodes:
  if [[ "${TESTS[@]}" =~ gluster ]] ; then # gluster test requested
    if [[ -z "$GLUSTER_NODES" ]] ; then
      echo "ERROR: missing list of gluster nodes"
      echo
      usage && return 1
    fi
    if [[ -z "$GLUSTER_VOL" ]] ; then
      echo "ERROR: missing the gluster volume"
      echo
      usage && return 1
    fi
    GLUSTER_NODES=(${GLUSTER_NODES//,/ }) # array
    # convert hostnames to IPs, if possible
    GLUSTER_NODES=($(convert_host_to_ip ${GLUSTER_NODES[@]}))
    NUM_ENDPOINTS=${#GLUSTER_NODES[@]}
  fi

  return 0
}

# outputs the starting SGID for the "default" namespace
function get_supplemental_group_id() {

  local out

  out="$(ssh $MASTER "${OC_PREFIX}oc get ns -o yaml |
           grep \"name: default\" -B6               |
           grep \"openshift.io/sa.scc.supplemental-groups:\"")"

  out="${out#*:}"   # keep the starting id and increment
  echo "${out%/*}" # the starting id

  return 0
}

# convert the passed-in list of potential hostnames to ip addresses.
# output the list with space delimiters so it is easily made into an array.
function convert_host_to_ip() {

  local list=($*) # array
  local ips=''; local node; local ip

  # nested function: returns true (0) if the passed-in name matches an ip-addr.
  function is_ip_addr() {

    local octet='(25[0-5]|2[0-4][0-9]|[01]?[0-9]?[0-9])' # cannot exceed 255
    local ipv4="^$octet\.$octet\.$octet\.$octet$"

    [[ "$1" =~ $ipv4 ]] && return 0 # true
    return 1 # false
  }

  # main #

  for node in ${list[*]}; do
     if is_ip_addr $node ; then
       ips+="$node "
     else
       ip="$(getent hosts $node)" # uses dns or /etc/hosts
       if (( $? != 0 )) || [[ -z "$ip" ]] ; then
         ips+="$node "
       else
         ips+="${ip%% *} " # just ip-addr
       fi
     fi
  done

  echo "$ips"
  return 0
}

# validate openshift env on passed-in node. Return 1 on error.
function validate_ose_env() {

  local out

  echo "*** Validating ose-master ($MASTER)..."
  echo

  ssh $MASTER "${OC_PREFIX}oc login -u admin -p ignored &&
               ${OC_PREFIX}oc project"
  (( $? != 0 )) && return 1
  
  # oc get nodes
  echo
  out="$(ssh $MASTER "${OC_PREFIX}oc get nodes")"
  if grep -q NotReady <<<"$out" ; then
    echo -e "ERROR: 1 or more nodes are not ready:\n$out"
    return 1
  fi

  # oc get scc
  echo
  out="$(ssh $MASTER "${OC_PREFIX}oc get scc | awk '{print \$4}'")" #HOSTDIR col
  if grep -q 'false' <<<"$out" ; then
    echo -e "ERROR: HOSTDIR is false:\n$out\nUse \"oc scc edit scc {privileged|restricted}\" and set allowHostDirVolumePlugin to true"
    return 1
  fi

  echo
  echo "... validated"
  echo
  return 0
}

# deletes the passed-in pod.
function delete_pod() {

  local pod=$1
  local i; local err; local out; local TRIES=10; local SLEEP=3

  # delete the target pod
  echo "... deleting pod \"$pod\" (if it exists)..."
  ssh -q $MASTER "${OC_PREFIX}oc delete pod $pod >&/dev/null"

  # wait for the delete to complete...
  for ((i=0; i<$TRIES; i++)); do
     sleep $SLEEP
     out="$(ssh $MASTER "${OC_PREFIX}oc get pod $pod 2>&1")"
     err=$?
     if (( err != 0 )) && 
        grep -q " not found" <<<"$out" ; then # expected err
       return 0
     fi
     if (( err != 0 )) ; then # unexpected
       echo "$out"
       return 1
     fi
  done

  echo -e "ERROR: $TRIES tries attempted to delete pod \"$pod\":\n$out"
  return 1
}

# deletes the passed-in pvc (claim).
function delete_pvc() {

  local pvc=$1
  local i; local err; local out; local TRIES=3; local SLEEP=3

  # delete the target pvc
  echo "... deleting pvc \"$pvc\" (if it exists)..."
  ssh -q $MASTER "${OC_PREFIX}oc delete pvc $pvc >&/dev/null"

  # wait for the delete to complete...
  for ((i=0; i<$TRIES; i++)); do
     sleep $SLEEP
     out="$(ssh $MASTER "${OC_PREFIX}oc get pvc $pvc 2>&1")"
     err=$?
     if (( err != 0 )) && 
        grep -q " not found" <<<"$out" ; then # expected err
       return 0
     fi
     if (( err != 0 )) ; then # unexpected
       echo "$out"
       return 1
     fi
  done

  echo -e "ERROR: $TRIES tries attempted to delete pvc \"$pvc\":\n$out"
  return 1
}

# deletes the passed-in pv (Persistent Volume).
function delete_pv() {

  local pv=$1
  local i; local err; local out; local TRIES=3; local SLEEP=3

  # delete the target pv
  echo "... deleting pv \"$pv\" (if it exists)..."
  ssh -q $MASTER "${OC_PREFIX}oc delete pv $pv >&/dev/null"

  # wait for the delete to complete...
  for ((i=0; i<$TRIES; i++)); do
     sleep $SLEEP
     out="$(ssh $MASTER "${OC_PREFIX}oc get pv $pv 2>&1")"
     err=$?
     if (( err != 0 )) && 
        grep -q " not found" <<<"$out" ; then # expected err
       return 0
     fi
     if (( err != 0 )) ; then # unexpected
       echo "$out"
       return 1
     fi
  done

  echo -e "ERROR: $TRIES tries attempted to delete pv \"$pv\":\n$out"
  return 1
}

# deletes the passed-in endpoint.
function delete_endpoint() {

  local endpt="$1"
  local i; local err; local out; local TRIES=3; local SLEEP=2

  # delete the target endpoint
  echo "... deleting endpoint \"$endpt\" (if it exists)..."
  ssh -q $MASTER "${OC_PREFIX}oc delete endpoints $endpt >&/dev/null"

  # wait for the delete to complete...
  for ((i=0; i<$TRIES; i++)); do
     sleep $SLEEP
     out="$(ssh $MASTER "${OC_PREFIX}oc get endpoints $endpt 2>&1")"
     err=$?
     if (( err != 0 )) && 
        grep -q " not found" <<<"$out" ; then # expected err
       return 0
     fi
     if (( err != 0 )) ; then # unexpected
       echo "$out"
       return 1
     fi
  done

  echo -e "ERROR: $TRIES tries attempted to delete endpoint \"$endpt\":\n$out"
  return 1
}

# returns 0 if the passed-in pod is running. Echos "oc get pod" output for all
# errors.
# $1= pod-name
function verify_new_pod() {

  local pod="$1"
  local i; local MAX=10 # num tries
  local SLEEP=4; local out

  echo "... checking pod \"$pod\" ..."

  for ((i=0; i<$MAX; i++)); do
     sleep $SLEEP
     out="$(ssh $MASTER "${OC_PREFIX}oc get pod $pod 2>&1 |
            awk '{print \$2}'")" # just the READY column
     (( $? != 0 )) && { 
       echo "$out"; return 1; }
     grep -q '1/1' <<<"$out" && return 0
  done

  echo -e "ERROR: $(((MAX*SLEEP))) seconds waiting for pod \"$pod\" to start:\n$out"
  return 1
}

# returns 0 if the passed-in PV is running.
function verify_new_pv() {

  local pv="$1"
  local i; local TRIES=3; local SLEEP=3; local out

  echo "... checking PV \"$pv\" ..."

  for ((i=0; i<$TRIES; i++)); do
     sleep $SLEEP
     out="$(ssh $MASTER "${OC_PREFIX}oc get pv $pv 2>&1 |
            awk '{print \$5}'")" # just the STATUS column
     (( $? != 0 )) && { 
       echo "$out"; return 1; }
     grep -q 'Available' <<<"$out" && return 0
  done

  echo -e "ERROR: $(((TRIES*SLEEP))) seconds waiting for PV \"$pv\" to start:\n$out"
  return 1
}

# returns 0 if the passed-in PVC is running.
function verify_new_pvc() {

  local pvc="$1"
  local i; local TRIES=3; local SLEEP=3; local out

  echo "... checking PVC \"$pvc\" ..."

  for ((i=0; i<$TRIES; i++)); do
     sleep $SLEEP
     out="$(ssh $MASTER "${OC_PREFIX}oc get pvc $pvc 2>&1 |
            awk '{print \$3}'")" # just the STATUS column
     (( $? != 0 )) && { 
       echo "$out"; return 1; }
     grep -q 'Bound' <<<"$out" && return 0
  done

  echo -e "ERROR: $(((TRIES*SLEEP))) seconds waiting for PVC \"$pvc\" to start:\n$out"
  return 1
}

# returns 0 if the passed-in endpoint is running.
function verify_new_endpoint() {

  local endpt="$1"
  local i; local TRIES=3; local SLEEP=3; local out

  echo "... checking endpoint \"$endpt\" ..."

  for ((i=0; i<$TRIES; i++)); do
     sleep $SLEEP
     out="$(ssh $MASTER "${OC_PREFIX}oc get endpoints $endpt 2>&1")"
     (( $? != 0 )) && { 
       echo "$out"; return 1; }
     grep -q "$endpt" <<<"$out" && return 0
  done

  echo -e "ERROR: $(((TRIES*SLEEP))) seconds waiting for endpoints \"$endpt\" to start:\n$out"
  return 1
}

# the various "test" functions are below. The general approach, in order to not
# need separate yaml files for each test's pod spec, is to supply the yaml as
# stdin to the oc create -f command.  Eg:
#   cat <<END | oc create -f -
#     <yaml here...>
#   END
#
# The tests below are based on this doc:
#   https://mojo.redhat.com/docs/DOC-1050225

# run the "general" tests.
function general_test() {

  local pod

  echo "*** General test suite ***"
  echo
  echo "   These tests test that SGID $SGID works in emptyDir and"
  echo "   in hostPath. No PVs or claims are used."
  sleep 2

  echo
  echo "----------"
  echo "General Test 2: busybox, emptyDir, SGID $SGID:"

  pod='general-pod2'
  delete_pod $pod

  cat <<END | ssh $MASTER "${OC_PREFIX}oc create -f -"
apiVersion: v1
kind: Pod
metadata:
  name: $pod
  labels:
    name: $pod
spec:
  containers:
  - name: $pod
    image: busybox
    command: ["sleep", "60000"]
  securityContext:
    supplementalGroups: [$SGID]
    privileged: false
END

  verify_new_pod $pod || {
    echo "ERROR: general test #2"; return 1; }

  echo
  echo "----------"
  echo "General Test 3: busybox, hostDir plugin, SGID $SGID:"

  pod='general-pod3'
  delete_pod $pod

  cat <<END | ssh $MASTER "${OC_PREFIX}oc create -f -"
apiVersion: v1
kind: Pod
metadata:
  name: $pod
  labels:
    name: $pod
spec:
  containers:
  - name: $pod
    image: busybox
    command: ["sleep", "60000"]
    volumeMounts:
    - name: bb-vol
      mountPath: /usr/share/busybox
  securityContext:
    supplementalGroups: [$SGID]
    privileged: false
  volumes:
  - name: bb-vol
    hostPath:
      path: /opt/data
END

  verify_new_pod $pod || {
    echo "ERROR: general test #3"; return 1; }

  return 0
}

# run the "nfs" tests.
function nfs_test() {

  local pod; local vol_mount='/opt/nfs'

  echo "*** NFS test suite ***"
  echo
  echo "   Remember to open port 2049 on the NFS server \"$NFS\"."
  echo "   A good test for this is, from the openshift node \"$MASTER\","
  echo "   execute:"
  echo "      \$ telnet $NFS 2049  # ctrl-c to exit"
  echo "   To open port 2049, on the NFS server, execute:"
  echo "      \$ iptables -I INPUT 1 -p tcp --dport 2049 -j ACCEPT"
  echo "   On the NFS server, edit /etc/exports to include /opt/nfs, eg:"
  echo "      /opt/nfs *(rw,sync,no_root_squash)"
  echo "   Also, on the NFS server, ensure that Group $SGID has"
  echo "   access to /opt/nfs. If not then execute:"
  echo "      \$ chgrp $SGID /opt/nfs && chmod g+srw /opt/nfs"
  echo "   It may necessary to restart NFS:"
  echo "      \$ systemctl restart rpcbind nfs"
  sleep 2

  echo
  echo "----------"
  echo "NFS Test 1: busybox, nfs plugin:"

  pod='nfs-pod1'
  delete_pod $pod

  cat <<END | ssh $MASTER "${OC_PREFIX}oc create -f -"
apiVersion: v1
kind: Pod
metadata:
  name: $pod
  labels:
    name: $pod
spec:
  containers:
  - name: $pod
    image: busybox
    command: ["sleep", "60000"]
    volumeMounts:
    - name: nfs-vol1
      mountPath: /usr/share/busybox
  securityContext:
      privileged: false
  volumes:
  - name: nfs-vol1
    nfs:
      path: "$vol_mount"
      server: $NFS
      readOnly: false
END

  if ! verify_new_pod $pod ; then
    echo "ERROR: nfs test #1"
    echo "       Try restarting rpcbind and nfs on the nfs server \"$NFS\"."
    echo "       Also make sure that port 2049 on the NFS server is open, eg:"
    echo "         iptables -I INPUT 1 -p tcp --dport 2049 -j ACCEPT"
    return 1
  fi

  echo
  echo "----------"
  echo "NFS Test 2: busybox, nfs plugin, SGID $SGID:"

  pod='nfs-pod2'
  delete_pod $pod

  cat <<END | ssh $MASTER "${OC_PREFIX}oc create -f -"
apiVersion: v1
kind: Pod
metadata:
  name: $pod
  labels:
    name: $pod
spec:
  containers:
  - name: $pod
    image: busybox
    command: ["sleep", "60000"]
    volumeMounts:
    - name: nfs-vol2
      mountPath: /usr/share/busybox
  securityContext:
    supplementalGroups: [$SGID]
    privileged: false
  volumes:
  - name: nfs-vol2
    nfs:
      path: "$vol_mount"
      server: $NFS
      readOnly: false
END

  verify_new_pod $pod || {
    echo "ERROR: nfs test #2"; return 1; }

  echo
  echo "----------"
  echo "NFS Test 3: busybox, nfs, PV, PVC, SGID $SGID:"

  pod='nfs-pod3'
  pv='nfs-pv'
  pvc='nfs-pvc'

  # cleanup
  delete_pod $pod
  delete_pvc $pvc
  delete_pv $pv

  # a. create PV
  cat <<END | ssh $MASTER "${OC_PREFIX}oc create -f -"
apiVersion: v1
kind: PersistentVolume
metadata:
  name: $pv
spec:
  capacity:
    storage: 1Gi
  accessModes:
  - ReadWriteOnce
  nfs:
    path: $vol_mount
    server: $NFS
END

  verify_new_pv $pv || {
    echo "ERROR: nfs test #3: can't create PV"; return 1; }

  # b. create PVC
  cat <<END | ssh $MASTER "${OC_PREFIX}oc create -f -"
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: $pvc
spec:
  accessModes:
  - ReadWriteOnce
  resources:
     requests:
       storage: 1Gi
END

  verify_new_pvc $pvc || {
    echo "ERROR: nfs test #3: can't create PVC"; return 1; }

  # c. create pod using claim
  cat <<END | ssh $MASTER "${OC_PREFIX}oc create -f -"
apiVersion: v1
kind: Pod
metadata:
  name: $pod
  labels:
    name: $pod
spec:
  containers:
  - name: $pod
    image: busybox
    command: ["sleep", "60000"]
    volumeMounts:
    - name: nfs-vol3
      mountPath: /usr/share/busybox
  securityContext:
    supplementalGroups: [$SGID]
    privileged: false
  volumes:
  - name: nfs-vol3
    persistentVolumeClaim:
      claimName: $pvc
END

  verify_new_pod $pod || {
    echo "ERROR: nfs test #3"; return 1; }

  return 0
}

# run the "glusterfs" tests.
function gluster_test() {

  local pod; local node; local endpoints=''
  local endpt='gluster-endpoints'

  echo "*** Gluster test suite ***"
  echo "   Uses the supplied gluster storage nodes (endpoints):"
  for node in ${GLUSTER_NODES[@]}; do
     echo "      $node"
  done
  echo "   and the glusterfs plugin to access \"$GLUSTER_VOL\"."
  sleep 2

  echo
  echo "----------"
  echo "Gluster Test 1: busybox, glusterfs plugin, SGID $SGID:"

  pod='gluster-pod1'
  delete_pod $pod
  delete_endpoint $endpt

  # a. create endpoints
  for node in ${GLUSTER_NODES[@]}; do # need newlines
     endpoints+="- addresses:"$'\n'
     endpoints+="  - IP: $node"$'\n'
     endpoints+="    ports:"$'\n'
     endpoints+="    - port: 1"$'\n' # ignored
     endpoints+="      protocol: TCP"$'\n'
  done

  cat <<END | ssh $MASTER "${OC_PREFIX}oc create -f -"
apiVersion: v1
kind: Endpoints
apiVersion: v1
metadata:
  name: $endpt
subsets:
$endpoints
END

  verify_new_endpoint $endpt || {
    echo "ERROR: gluster test #1: can't create endpoints"; return 1; }

  # b. create pod
  cat <<END | ssh $MASTER "${OC_PREFIX}oc create -f -"
apiVersion: v1
kind: Pod
metadata:
  name: $pod
  labels:
    name: $pod
spec:
  containers:
  - name: $pod
    image: busybox
    command: ["sleep", "60000"]
    volumeMounts:
    - name: gluster-vol1
      mountPath: /usr/share/busybox
  securityContext:
    supplementalGroups: [$SGID]
    privileged: false
  volumes:
  - name: gluster-vol1
    glusterfs:
      endpoints: $endpt
      path: $GLUSTER_VOL
      readOnly: false
END

  if ! verify_new_pod $pod ; then
    echo "ERROR: gluster test #1"
    echo "       Try ..."
    return 1
  fi

  return 0
}

# execute the passed-in test(s).
# $@= list of 1 or more tests (space separator)
function execute_tests() {

  local tests="$@"
  local test; local errcnt=0

  echo "*** Executing tests ..."

  for test in $tests; do
     echo
     case "$test" in
        general)
          general_test || ((errcnt++))
        ;;

        nfs)
          nfs_test || ((errcnt++))
        ;;

        gluster)
          gluster_test || ((errcnt++))
        ;;

        rbd)
          rbd_test || ((errcnt++))
        ;;

        *)
          echo "ERROR: unknown test \"$test\"" && ((errcnt++))
        ;;
     esac
  done

  echo
  echo "... done with tests: $errcnt errors"
  echo

  (( errcnt > 0 )) && return 1
  return 0
}


## main ##

parse_cmd $@ || exit -1

echo
if (( VERIFY_ONLY )) ; then
  echo "*** Only validating the environment on ose-master \"$MASTER\""
else
  (( NUM_TESTS > 1 )) && plural='s' || plural=''
  echo "*** Will run $NUM_TESTS test$plural on ose-master \"$MASTER\":"
  for test in ${TESTS[@]}; do
     echo "       $test"
  done
fi
echo
sleep 2

validate_ose_env || exit 1

if (( ! VERIFY_ONLY )) ; then
  SGID=$(get_supplemental_group_id)
  execute_tests ${TESTS[@]} || exit 1
fi

exit 0
