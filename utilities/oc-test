#!/bin/bash
#
# oc-test creates ose pods on the ose-master node (defaults to localhost) and
# runs one or more of the specified storage tests.
#
# The storage specific test functions are named xxx_test(), eg. general_test()
# or nfs_test(). The general approach, in order to not need separate yaml files
# for each test's pod spec, is to supply the yaml as stdin to the:
# oc create -f command.  Eg:
#   cat <<END | oc create -f -
#     <yaml here...>
#   END
#
# The storage tests are loosley based on Scott Creeley's doc:
#   https://mojo.redhat.com/docs/DOC-1050225

# See usage() for syntax.
#

VERSION=0.9
VERIFY_ONLY=0 # false
MASTER="$HOSTNAME"
OC_PREFIX=''
# test vars
ALL_TESTS='general,nfs,gluster,rbd'
TESTS='general' # default
GENERAL_TEST=1 # true, default
GLUSTER_TEST=0 # false
NFS_TEST=0     # false
RBD_TEST=0     # false


## functions ##

function usage() {

  cat <<END
  oc-test - execute one or more ose storage tests

SYNTAX
  oc-test [test-name] [options]

DESCRIPTION
  Verifies the ose-master environment and, optionally, executes one or more
  storage tests on the ose-master server.

  test-name  A list of one or more tests to run. Expected values are:
               general (default),
               nfs,
               gluster,
               rbd (or ceph),
               all.
             More than one test is specified with a comma separator, eg.
             "nfs,gluster".

  --verify   Only test the ose enviromment but don't run any tests.
  --master <node>
             Hostname or ip of the ose-master node, default is local host
  --oc-prefix <path>
             An optional path prefix appended to all "oc" commands, eg.
             "/root/origin/_output/local/bin/linux/amd64" for local ose builds.
  --sgid <number>
             An optional supplemental group ID to be applied to the pods created
             by various tests. If omitted the default is the first id in the
             supplemental group ID range defined for the current project. See:
               $ oc get ns <project> -o yaml  # for the various IDs
  --nfs-server <hostname|ip>
             NFS server's hostname or ip. Required if performing the nfs tests,
             otherwise ignored.
  --gluster-nodes <node-list>
             A list of 2 or more gluster storage node IP addresses (comma
             separator), which become the ose endpoints. Hostnames are converted
             to IPs using getent, but if DNS or /etc/hosts is not setup
             correctly it is better to supply IPs. Required if performing the
             gluster tests, otherwise ignored. Eg.:
               192.168.122.21,192.168.122.22
  --gluster-vol <volName>
             The name of an existing gluster/RHS volume. Required if performing
             the gluster tests, otherwise ignored.
  --rbd-monitors <node-list>
             A list of 1 or more ceph Monitor IP addresses (comma separated).
             Hostnames are converted to IPs using getent, but if DNS or
             /etc/hosts is not setup correctly it is better to supply the IP.
             If the default port of 6789 is not used then :portNum must follow
             all IPs not using the default port.  Required if performing the 
             ceph-rbd tests, otherwise ignored. Eg:
               192.168.122.133:6788,192.168.122.134
  --ceph-secret64 <base64 value>
             The base-64 encoded secret string generated by running:
               ceph auth get-key client.admin  , followed by pasting that output
             to the base64 command:
               echo -n "<output-from-ceph-auth-get-key>" | base64
             If omitted and the rbd test is requested and a provided ceph 
             monitor is reachable via ssh, then this value is calculated by the
             rbd test and is not required. It is ignore by all tests other than
             the ceph-rbd tests.
  --rbd-image <name>
             The name of the ceph-rbd image. If the rbd pool is not defaulted to
             "rbd" then the pool name is also required in the form of:
               <pool-name>/<image-name>
             Ignored for all tests other than the ceph-rbd tests.
  --version  Show the version string.
  -q         Suppress all prompts and reduce instructional output.

END

}

# output the list of tests with commas replaced by spaces so it is suitable
# for use as an array. Note: ceph and rbd are synonyms for the same test and
# therefore ceph is changed to rbd. Returns 1 for errors.
# $1= a list of 1 or more tests, comma separated.
function parse_tests() {

  local tests="${1/ceph/rbd}"
  local test

  tests="${tests//,/ }"

  # validate known test names
  for test in $tests; do
     if [[ ! "$ALL_TESTS" =~ "$test" ]] ; then
       echo "ERROR: unknown test \"$test\""
       return 1
     fi
  done

  echo "$tests"
  return 0
}

# set globals to true for each test passed in.
function set_test_flags() {

  local tests="$@"
  local test

  for test in $tests; do
     case "$test" in
        general)
          GENERAL_TEST=1 # true
        ;;
        gluster)
          GLUSTER_TEST=1
        ;;
        nfs)
          NFS_TEST=1
        ;;
        rbd|ceph)
          RBD_TEST=1
        ;;
     esac

  done

  return 0
}

# returns 1 on errors, else 0.
function parse_cmd() {

  local errcnt=0
  local opts='q'
  local long_opts='version,master:,verify,oc-prefix:,nfs-server:,gluster-nodes:,gluster-vol:,sgid:,rbd-monitors:,ceph-secret64:,rbd-image:'

  (( $# == 0 )) && usage && exit -1

  eval set -- "$(getopt -o "$opts" --long $long_opts -- $@)"

  while true; do
      case "$1" in
        -q)
          QUIET=1 # true
          shift; continue
        ;;

        --version)
          echo $VERSION
          exit 0
        ;;

        --verify)
          VERIFY_ONLY=1; # true
          shift; continue
        ;;

        --master)
          MASTER="$2"
          shift 2; continue
        ;;

        --sgid)
          TGT_SGID="$2"
          shift 2; continue
        ;;

        --oc-prefix)
          OC_PREFIX="$2"
          shift 2; continue
        ;;

        --nfs-server)
          NFS="$2"
          shift 2; continue
        ;;

        --gluster-nodes)
          GLUSTER_NODES="$2" #comma separated list
          shift 2; continue
        ;;

        --gluster-vol)
          GLUSTER_VOL="$2"
          shift 2; continue
        ;;

        --rbd-monitors)
          MONITORS="$2" #comma separated list if more than 1
          shift 2; continue
        ;;

        --ceph-secret64)
          CEPH_SECRET64="$2"
          shift 2; continue
        ;;

        --rbd-image)
          RBD_IMAGE="$2"
          shift 2; continue
        ;;

        --)
          shift; break
        ;;
      esac
  done

  if (( $# > 1 )) ; then
    shift
    echo "Syntax error: unexpected command line arg: $@"
    return 1
  fi

  if (( $# == 1 )) ; then # test arg is optional; default is general tests
    GENERAL_TEST=0 # false, don't assume general is in the list of tests
    TESTS="$1"; shift
    # handle "all" tests
    if [[ "$TESTS" == 'all' ]] ; then
      TESTS="$ALL_TESTS"
    fi
  fi

  # handle a list of tests
  TESTS="$(parse_tests "$TESTS")"
  if (( $? != 0 )) ; then
    echo "$TESTS" # error msg
    return -1
  fi
  set_test_flags $TESTS
  TESTS=($TESTS)  # array
  NUM_TESTS=${#TESTS[@]}

  # --oc-prefix:
  if [[ -n "$OC_PREFIX" ]] ; then
    # oc prefix must end in a '/'
    [[ "$OC_PREFIX" != */ ]] && OC_PREFIX="$OC_PREFIX/"
    # make sure oc prefix exists as a dir on the master node
    if ! ssh $MASTER "[[ -d $OC_PREFIX ]]" ; then
      echo "ERROR: no directory named \"$OC_PREFIX\" on master node ($MASTER)"
      return 1
    fi
  fi

  # --nfs-server:
  if (( NFS_TEST )) ; then
    if [[ -z "$NFS" ]] ; then
      echo "ERROR: missing nfs server argument"
      echo
      usage && return 1
    fi
  fi

  # --gluster-nodes:
  if (( GLUSTER_TEST )) ; then
    if [[ -z "$GLUSTER_NODES" ]] ; then
      echo "ERROR: missing list of gluster nodes"
      echo
      usage && return 1
    fi
    if [[ -z "$GLUSTER_VOL" ]] ; then
      echo "ERROR: missing the gluster volume"
      echo
      usage && return 1
    fi
    # convert hostnames to IPs, if possible
    GLUSTER_NODES=($(convert_host_to_ip ${GLUSTER_NODES//,/ })) # array
  fi

  # --rbd-monitors:
  if (( RBD_TEST )) ; then
    if [[ -z "$MONITORS" ]] ; then
      echo "ERROR: missing RBD monitor node(s)"
      echo
      usage && return 1
    fi
    if [[ -z "$RBD_IMAGE" ]] ; then
      echo "ERROR: missing the RBD image name"
      echo
      usage && return 1
    fi
    # convert hostnames to IPs, if possible
    MONITORS=($(convert_host_to_ip ${MONITORS//,/ })) # array
  fi

  return 0
}

# returns true (0) if the passed-in id is within (inclusive) the pased-in range.
#   arg1=test-id, arg2=range.
# Ranges are of the form: <startID>/<count>  or <startID>-<endID>  and
# a list of ranges are separated by a comma (no spaces).  Eg.
#   range-1,range-2,...range-N
function id_in_range() {

  local test_id=$1; local range="${2//,/ }" # list
  local id; local r; local start; local end; local cnt

  for r in ${range[@]}; do
     if [[ $r == *"/"* ]] ; then # <start>/<count> form
       start=${r%/*}; cnt=${r#*/}; let end=start+cnt-1
     else # <start>-<end> form
       start=${r%-*}; end=${r#*-}
     fi
     # test if test_id is within this range
     (( start <= test_id && test_id <= end )) && return 0 # true
  done

  return 1 # false
}

# sets the SGIDs, SUIDs, OS_SGID, OS_SUID and the USE_SGID global variables to
# the values found in the project definition for the passed-in namespace. 
# Note: ID ranges have 2 formats: <start/cnt> or <start>-<end>
function get_supplemental_ids() {

  local ns="$1"
  local out; local ids; local id

  out="$(ssh $MASTER "${OC_PREFIX}oc get ns $ns -o yaml")"

  # SGIDs:
  ids="$(grep 'openshift.io/sa.scc.supplemental-groups:' <<<"$out")"
  SGIDs="${ids#*: }"  # remove the property name
  id="${SGIDs%%,*}"   # first range

  if [[ $id == *"/"* ]] ; then # <start>/<count> form
    OS_SGID=${id%/*} # first number in range
  else # <start>-<end> form
    OS_SGID=${id%-*} # first number in range
  fi

  # use either the supplied --sgid= value or the openshift default
  USE_SGID=${TGT_SGID:-$OS_SGID}
  # error out if the target sgid is not w/in the openshift range
  if ! id_in_range $USE_SGID "$SGIDs" ; then
    echo "ERROR: target SGID of $USE_SGID is not within the range of supplemental"
    echo "       group IDs defined for the \"$NS\" project. Openshift's SGID"
    echo "       range is: $SGIDs. Either supply a different --sgid= value or"
    echo "       change the 'openshift.io/sa.scc.supplemental-groups' range"
    echo "       using the \"oc edit ns $NS\" command."
    return 1
  fi

  # SUIDs:
  ids="$(grep 'openshift.io/sa.scc.uid-range:' <<<"$out")"
  SUIDs="${ids#*: }"  # remove the property name
  id="${SUIDs%%,*}"   # first range

  if [[ $id == *"/"* ]] ; then # <start>/<count> form
    OS_SUID=${id%/*} # first number in range
  else # <start>-<end> form
    OS_SUID=${id%-*} # first number in range
  fi

  return 0
}

# Convert the passed-in list of possible hostnames to ip addresses.
# Output the list with space delimiters so it is easily made into an array.
# Note: the list of hostnames may include :portNum after each IP/hostname,
#   which is preserved.
function convert_host_to_ip() {

  local list=($*) # array
  local ips=''; local node; local ip; local port

  # nested function: returns true (0) if the passed-in name matches an ip-addr.
  function is_ip_addr() {

    local octet='(25[0-5]|2[0-4][0-9]|[01]?[0-9]?[0-9])' # cannot exceed 255
    local ipv4="^$octet\.$octet\.$octet\.$octet$"

    [[ "$1" =~ $ipv4 ]] && return 0 # true
    return 1 # false
  }

  # main #

  for node in ${list[*]}; do
     # check for optional :port
     port=''
     if [[ "${node%:*}" != "$node" ]] ; then # port
       port="${node#*:}"; node="${node%:*}"
     fi

     if is_ip_addr $node ; then
       ips+="$node"
     else
       ip="$(getent hosts $node)" # uses dns or /etc/hosts
       if (( $? != 0 )) || [[ -z "$ip" ]] ; then
         ips+="$node"
       else
         ips+="${ip%% *}" # just ip-addr
       fi
     fi
     # remember port, if present
     [[ -n "$port" ]] && ips+=":$port"
     # append trailing space separator
     ips+=' '
  done

  echo "$ips"
  return 0
}

# echos the current namespace (=project) without any quotemarks.
function get_current_namespace() {

  local ns

  ns=($(ssh $MASTER "${OC_PREFIX}oc project")) # array
  ns=${ns[2]} # includes quotes
  ns=${ns%\"}; ns=${ns#\"} # remove enclosing quotes

  echo "$ns"
  return 0
}

# validate openshift env on passed-in node. Return 1 on error. If no 
# validation errors then collected ose state variables are echoed. 
function validate_ose_env() {

  local out

  echo "*** Validating ose-master: \"$MASTER\"..."
  echo

  ssh $MASTER "${OC_PREFIX}oc login -u admin -p ignored"
  (( $? != 0 )) && return 1
  
  # oc get nodes
  out="$(ssh $MASTER "${OC_PREFIX}oc get nodes")"
  if grep -q NotReady <<<"$out" ; then
    echo -e "ERROR: 1 or more nodes are not ready:\n$out"
    return 1
  fi

  # oc get scc
  out="$(ssh $MASTER "${OC_PREFIX}oc get scc")"
  if awk '{print $4}' <<<"$out" | grep -q false ; then #HOSTDIR col has a false
    echo "ERROR: HOSTDIR is false:"
    echo "$out"
    echo
    echo "Use the \"oc edit scc {privileged|restricted}\" command and set"
    echo "'allowHostDirVolumePlugin' to true."
    return 1
  fi

  echo "... validated"
  echo
  echo "==================================="
  echo " Master node     : $MASTER"
  echo " Current project : $NS"
  echo "   Sup User IDs  : $SUIDs"
  echo "   Sup Group IDs : $SGIDs"
  echo " Supplied Sup GID: ${TGT_SGID:-<none>}"
  echo " Pod's Group ID  : $USE_SGID"
  echo "==================================="
  echo

  return 0
}

# deletes the passed-in pod.
function delete_pod() {

  local pod=$1
  local i; local err; local out; local TRIES=10; local SLEEP=3

  # delete the target pod
  echo "... deleting pod \"$pod\" (if it exists)..."
  ssh -q $MASTER "${OC_PREFIX}oc delete pod $pod >&/dev/null"

  # wait for the delete to complete...
  for ((i=0; i<$TRIES; i++)); do
     sleep $SLEEP
     out="$(ssh $MASTER "${OC_PREFIX}oc get pod $pod 2>&1")"
     err=$?
     if (( err != 0 )) && 
        grep -q " not found" <<<"$out" ; then # expected err
       return 0
     fi
     if (( err != 0 )) ; then # unexpected
       echo "$out"
       return 1
     fi
  done

  echo -e "ERROR: $TRIES tries attempted to delete pod \"$pod\":\n$out"
  return 1
}

# deletes the passed-in pvc (claim).
function delete_pvc() {

  local pvc=$1
  local i; local err; local out; local TRIES=3; local SLEEP=3

  # delete the target pvc
  echo "... deleting pvc \"$pvc\" (if it exists)..."
  ssh -q $MASTER "${OC_PREFIX}oc delete pvc $pvc >&/dev/null"

  # wait for the delete to complete...
  for ((i=0; i<$TRIES; i++)); do
     sleep $SLEEP
     out="$(ssh $MASTER "${OC_PREFIX}oc get pvc $pvc 2>&1")"
     err=$?
     if (( err != 0 )) && 
        grep -q " not found" <<<"$out" ; then # expected err
       return 0
     fi
     if (( err != 0 )) ; then # unexpected
       echo "$out"
       return 1
     fi
  done

  echo -e "ERROR: $TRIES tries attempted to delete pvc \"$pvc\":\n$out"
  return 1
}

# deletes the passed-in pv (Persistent Volume).
function delete_pv() {

  local pv=$1
  local i; local err; local out; local TRIES=15; local SLEEP=2

  # delete the target pv
  echo "... deleting pv \"$pv\" (if it exists)..."
  ssh -q $MASTER "${OC_PREFIX}oc delete pv $pv >&/dev/null"

  # wait for the delete to complete...
  for ((i=0; i<$TRIES; i++)); do
     sleep $SLEEP
     out="$(ssh $MASTER "${OC_PREFIX}oc get pv $pv 2>&1")"
     err=$?
     if (( err != 0 )) && 
        grep -q " not found" <<<"$out" ; then # expected err
       return 0
     fi
     if (( err != 0 )) ; then # unexpected
       echo "$out"
       return 1
     fi
  done

  echo -e "ERROR: $TRIES tries attempted to delete pv \"$pv\":\n$out"
  return 1
}

# deletes the passed-in endpoint.
function delete_endpoint() {

  local endpt="$1"
  local i; local err; local out; local TRIES=3; local SLEEP=2

  # delete the target endpoint
  echo "... deleting endpoint \"$endpt\" (if it exists)..."
  ssh -q $MASTER "${OC_PREFIX}oc delete endpoints $endpt >&/dev/null"

  # wait for the delete to complete...
  for ((i=0; i<$TRIES; i++)); do
     sleep $SLEEP
     out="$(ssh $MASTER "${OC_PREFIX}oc get endpoints $endpt 2>&1")"
     err=$?
     if (( err != 0 )) && 
        grep -q " not found" <<<"$out" ; then # expected err
       return 0
     fi
     if (( err != 0 )) ; then # unexpected
       echo "$out"
       return 1
     fi
  done

  echo -e "ERROR: $TRIES tries attempted to delete endpoint \"$endpt\":\n$out"
  return 1
}

# deletes the passed-in secret.
function delete_secret() {

  local secret="$1"
  local i; local err; local out; local TRIES=3; local SLEEP=2

  # delete the target secret
  echo "... deleting secret \"$secret\" (if it exists)..."
  ssh -q $MASTER "${OC_PREFIX}oc delete secret $secret >&/dev/null"

  # wait for the delete to complete...
  for ((i=0; i<$TRIES; i++)); do
     sleep $SLEEP
     out="$(ssh $MASTER "${OC_PREFIX}oc get secret $secret 2>&1")"
     err=$?
     if (( err != 0 )) && 
        grep -q " not found" <<<"$out" ; then # expected err
       return 0
     fi
     if (( err != 0 )) ; then # unexpected
       echo "$out"
       return 1
     fi
  done

  echo -e "ERROR: $TRIES tries attempted to delete secret \"$secret\":\n$out"
  return 1
}

# returns 0 if the passed-in pod's container is missing the passed-in mount
# type (eg. rbd, glusterfs).
#   $1=mount type, $2= pod-name
# Note: this function should only be called after the target pod as been
#   verified to be Running.
function verify_mount() {

  local mnt="$1"; local pod="$2"
  local out

  echo "... checking mount type \"$mnt\" for pod \"$pod\" ..."

  # first check mount inside container
  out="$(ssh $MASTER "${OC_PREFIX}oc exec $pod mount | grep $mnt 2>&1")"
  if (( $? == 0 )) && [[ -n "$out" ]] &&
     grep -q '/usr/share/busybox' <<<"$out" ; then
    return 0 # success
  fi

  echo -e "ERROR: $mnt mount missing in pod $pod:\n$out"
  return 1
}

# returns 0 if the passed-in pod is running. Echos "oc get pod" output for all
# errors.
# $1= pod-name
function verify_new_pod() {

  local pod="$1"
  local i; local MAX=60 # num tries
  local SLEEP=3; local out

  echo "... checking pod \"$pod\" ..."

  for ((i=0; i<$MAX; i++)); do
     sleep $SLEEP
     out="$(ssh $MASTER "${OC_PREFIX}oc get pod $pod 2>&1")"
     (( $? != 0 )) && {
       echo "$out"; return 1; }
     if awk '{print $2}' <<<"$out" | grep -q '1/1' ; then # READY column
       return 0 # success
     fi
  done

  echo -e "ERROR: $(((MAX*SLEEP))) seconds waiting for pod \"$pod\" to start:\n$out"
  return 1
}

# returns 0 if the passed-in PV is running.
function verify_new_pv() {

  local pv="$1"
  local i; local TRIES=60; local SLEEP=2; local out

  echo "... checking PV \"$pv\" ..."

  for ((i=0; i<$TRIES; i++)); do
     sleep $SLEEP
     out="$(ssh $MASTER "${OC_PREFIX}oc get pv $pv 2>&1")"
     (( $? != 0 )) && { 
       echo "$out"; return 1; }
     if awk '{print $5}' <<<"$out" | grep -q 'Available' ; then # STATUS column
       return 0 # success
     fi
  done

  echo -e "ERROR: $(((TRIES*SLEEP))) seconds waiting for PV \"$pv\" to start:\n$out"
  return 1
}

# returns 0 if the passed-in PVC is running.
function verify_new_pvc() {

  local pvc="$1"
  local i; local TRIES=30; local SLEEP=2; local out

  echo "... checking PVC \"$pvc\" ..."

  for ((i=0; i<$TRIES; i++)); do
     sleep $SLEEP
     out="$(ssh $MASTER "${OC_PREFIX}oc get pvc $pvc 2>&1")"
     (( $? != 0 )) && { 
       echo "$out"; return 1; }
     if awk '{print $3}' <<<"$out" | grep -q 'Bound' ; then # STATUS column
       return 0 # success
     fi
  done

  echo -e "ERROR: $(((TRIES*SLEEP))) seconds waiting for PVC \"$pvc\" to start:\n$out"
  return 1
}

# returns 0 if the passed-in endpoint is running.
function verify_new_endpoint() {

  local endpt="$1"
  local i; local TRIES=3; local SLEEP=3; local out

  echo "... checking endpoint \"$endpt\" ..."

  for ((i=0; i<$TRIES; i++)); do
     sleep $SLEEP
     out="$(ssh $MASTER "${OC_PREFIX}oc get endpoints $endpt 2>&1")"
     (( $? != 0 )) && { 
       echo "$out"; return 1; }
     grep -q "$endpt" <<<"$out" && return 0
  done

  echo -e "ERROR: $(((TRIES*SLEEP))) seconds waiting for endpoints \"$endpt\" to start:\n$out"
  return 1
}

# returns 0 if the passed-in secret is running.
function verify_new_secret() {

  local secret="$1"
  local i; local TRIES=3; local SLEEP=2; local out

  echo "... checking secret \"$secret\" ..."

  for ((i=0; i<$TRIES; i++)); do
     sleep $SLEEP
     out="$(ssh $MASTER "${OC_PREFIX}oc get secret $secret 2>&1")"
     (( $? != 0 )) && { 
       echo "$out"; return 1; }
     grep -q 'Opaque' <<<"$out" && return 0
  done

  echo -e "ERROR: $(((TRIES*SLEEP))) seconds waiting for secret \"$secret\" to start:\n$out"
  return 1
}

# run the "general" tests.
function general_test() {

  local pod

  if (( ! QUIET )) ; then
    cat <<END_GENERAL
*** General test suite ***"

    These baseline tests test that SGID $SUSE_GID works in emptyDir and in
    hostPath. No PVs or claims are used here.

END_GENERAL

    read -p "Press any key to continue..." -t120 out
  fi

  echo
  echo "----------"
  echo "General Test 1: busybox, emptyDir, SGID $USE_SGID:"

  pod='general-pod1'
  delete_pod $pod

  cat <<END | ssh $MASTER "${OC_PREFIX}oc create -f -"
apiVersion: v1
kind: Pod
metadata:
  name: $pod
  labels:
    name: $pod
spec:
  containers:
  - name: $pod
    image: busybox
    command: ["sleep", "60000"]
  securityContext:
    supplementalGroups: [$USE_SGID]
    privileged: false
END

  verify_new_pod $pod || {
    echo "ERROR: general test #1"; return 1; }

  echo
  echo "----------"
  echo "General Test 2: busybox, hostPath plugin, SGID $USE_SGID:"

  pod='general-pod2'
  delete_pod $pod

  cat <<END | ssh $MASTER "${OC_PREFIX}oc create -f -"
apiVersion: v1
kind: Pod
metadata:
  name: $pod
  labels:
    name: $pod
spec:
  containers:
  - name: $pod
    image: busybox
    command: ["sleep", "60000"]
    volumeMounts:
    - name: bb-vol
      mountPath: /usr/share/busybox
  securityContext:
    supplementalGroups: [$USE_SGID]
    privileged: false
  volumes:
  - name: bb-vol
    hostPath:
      path: /opt/data
END

  verify_new_pod $pod || {
    echo "ERROR: general test #2"; return 1; }

  return 0
}

# run the "nfs" tests.
# Note: the user-specified --sgid is used in these tests. If omitted then the
#  default openshift sgid is used.
function nfs_test() {

  local pod; local pv; local pvc; local out
  local vol_mount='/opt/nfs'; local vol_perms; local vol_gid
  local sup_gids=$USE_SGID # may have the nfs vol's gid appended

  echo
  echo "*** NFS test suite ***"
  echo

  # get vol mount perms from nfs server
  echo "Connecting to $NFS via ssh. You may need to enter a password."
  sleep 1
  echo

  out="$(ssh $NFS "ls -ld $vol_mount 2>&1")"
  if (( $? == 0 )) ; then
    out=($out) # array
    vol_perms="${out[0]}"
    vol_gid="${out[3]}"

    # handle string gid
    if [[ ! $vol_gid =~ ^[0-9]+$ ]] ; then # not all numeric
echo "***String nfs vol id $vol_gid"
      # get the numeric id from the NFS server
      echo "Converting \"$vol_gid\" to its numeric value on $node..."
      out="$(ssh $NFS "getent group $vol_gid | cut -d: -f3")"
      if (( $? == 0 )) ; then
        vol_gid="$out"
      else
        echo "WARN: unable to convert the NFS mount directory ${vol_mount}'s"
        echo "  string GID \"$vol_gid\" to its numeric value. OSE doesn't"
        echo "  support string GIDs in its ID range implementation. The mount's"
        echo "  GID will be set to \"\" (nil)."
        vol_gid=''
      fi
    fi

  else
    echo "WARN: cannot retrieve permissions on $vol_mount from NFS server"
    echo "  \"$NFS\" via ssh. Be aware of potential permission mismatches"
    echo "  between openshift supplemental groups and the NFS mount directory"
    echo
    echo "$out"
  fi

  # append nfs vol's sgid to pod's sgid?
  if [[ -n "$vol_gid" ]] ; then
    if id_in_range $vol_gid "$SGIDs" ; then
      if (( vol_gid != USE_SGID )) ; then # append vol gid to supp gids
        sup_gids+=",$vol_gid"     
        echo "Pod's supplemental groups are: $sup_gids"
      fi
    else
      echo "WARN: the NFS volume ${vol_mount}'s GID of $vol_gid is NOT within"
      echo "  the \"$NS\" project's range of group IDs: $SGIDs, and"
      echo "  thus cannot be added to the pod's list of group IDs."
    fi
  fi

  # show more info for running these tests...
  if (( ! QUIET )) ; then
    cat <<END_NFS

    Busybox is run with a volume mounted to the NFS server: $NFS.
    Remember to open port 2049 on the NFS server. A good test for this is,
    from the openshift-master, "$MASTER", execute:
       $ telnet $NFS 2049  # ctrl-c to exit
    To open port 2049 execute (on the NFS server):
       $ iptables -I INPUT 1 -p tcp --dport 2049 -j ACCEPTA

    Also, on the NFS server, edit /etc/exports to include /opt/nfs, eg:
       /opt/nfs *(rw,sync,no_root_squash)

    The group ID for the NFS export directory $vol_mount is:
       ${vol_gid:-<unavailable>}
    and permissions on this directory are:
       ${vol_perms:-<unavailable>}

    To edit the range of supplemental group IDs, on the openshift-master, use:
       $ oc edit ns $NS
    and change the 'openshift.io/sa.scc.supplemental-groups' range to
    include the NFS group ID for $vol_mount.  Also, use:
       $ oc get ns $NS -o yaml
    to see the values for various IDs in the "$NS" project.

    On the other hand, if it's ok to change the perms on $vol_mount to match
    openshift's range of groups, then execute (on $NFS):
       $ chgrp $USE_SGID /opt/nfs

    Note: it may necessary to restart NFS:
       $ systemctl restart rpcbind nfs

END_NFS

    read -p "Press any key to continue..." -t120 out
  fi

  echo
  echo "----------"
  echo "NFS Test 1: baseline: busybox, nfs plugin:"

  pod='nfs-pod1'
  delete_pod $pod

  cat <<END | ssh $MASTER "${OC_PREFIX}oc create -f -"
apiVersion: v1
kind: Pod
metadata:
  name: $pod
  labels:
    name: $pod
spec:
  containers:
  - name: $pod
    image: busybox
    command: ["sleep", "60000"]
    volumeMounts:
    - name: nfs-vol1
      mountPath: /usr/share/busybox
  securityContext:
      privileged: false
  volumes:
  - name: nfs-vol1
    nfs:
      path: "$vol_mount"
      server: $NFS
      readOnly: false
END

  if ! verify_new_pod $pod ; then
    echo "ERROR nfs test 1: pod $pod not Running"
    echo "  Try restarting rpcbind and nfs on the nfs server \"$NFS\"."
    echo "  Also make sure that port 2049 on the NFS server is open, eg:"
    echo "    iptables -I INPUT 1 -p tcp --dport 2049 -j ACCEPT"
    return 1
  fi

  verify_mount nfs $pod || {
    echo "ERROR nfs test 1: expected glusterfs mount missing"; return 1; }

  echo
  echo "----------"
  echo "NFS Test 2: busybox, nfs plugin, SGID $sup_gids:"

  pod='nfs-pod2'
  delete_pod $pod

  cat <<END | ssh $MASTER "${OC_PREFIX}oc create -f -"
apiVersion: v1
kind: Pod
metadata:
  name: $pod
  labels:
    name: $pod
spec:
  containers:
  - name: $pod
    image: busybox
    command: ["sleep", "60000"]
    volumeMounts:
    - name: nfs-vol2
      mountPath: /usr/share/busybox
  securityContext:
    supplementalGroups: [$sup_gids]
    privileged: false
  volumes:
  - name: nfs-vol2
    nfs:
      path: "$vol_mount"
      server: $NFS
      readOnly: false
END

  verify_new_pod $pod || {
    echo "ERROR nfs test 2: pod $pod not Running"; return 1; }

  verify_mount nfs $pod || {
    echo "ERROR nfs test 2: expected glusterfs mount missing"; return 1; }

  echo
  echo "----------"
  echo "NFS Test 3: busybox, PV, PVC, SGID $sup_gids:"

  # a. create PV
  pv='nfs-pv'
  pvc='nfs-pvc'
  # delete the pvc before creating the pv. That way verify_new_pv() will see
  # the "Available" status, and not "Bound" due to the existing PVC binding to
  # the new PV before the script sees "Available"
  delete_pvc $pvc
  delete_pv $pv

  cat <<END | ssh $MASTER "${OC_PREFIX}oc create -f -"
apiVersion: v1
kind: PersistentVolume
metadata:
  name: $pv
spec:
  capacity:
    storage: 1Gi
  accessModes:
  - ReadWriteOnce
  nfs:
    path: $vol_mount
    server: $NFS
END

  verify_new_pv $pv || {
    echo "ERROR: nfs test #3: can't create PV"; return 1; }

  # b. create PVC
  cat <<END | ssh $MASTER "${OC_PREFIX}oc create -f -"
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: $pvc
spec:
  accessModes:
  - ReadWriteOnce
  resources:
     requests:
       storage: 1Gi
END

  verify_new_pvc $pvc || {
    echo "ERROR: nfs test #3: can't create PVC"; return 1; }

  # c. create pod using claim
  pod='nfs-pod3'
  delete_pod $pod

  cat <<END | ssh $MASTER "${OC_PREFIX}oc create -f -"
apiVersion: v1
kind: Pod
metadata:
  name: $pod
  labels:
    name: $pod
spec:
  containers:
  - name: $pod
    image: busybox
    command: ["sleep", "60000"]
    volumeMounts:
    - name: nfs-vol3
      mountPath: /usr/share/busybox
  securityContext:
    supplementalGroups: [$sup_gids]
    privileged: false
  volumes:
  - name: nfs-vol3
    persistentVolumeClaim:
      claimName: $pvc
END

  verify_new_pod $pod || {
    echo "ERROR nfs test 3: pod $pod not Running"; return 1; }

  verify_mount nfs $pod || {
    echo "ERROR nfs test 3: expected glusterfs mount missing"; return 1; }

  return 0
}

# run the "glusterfs" tests.
function gluster_test() {

  local pod; local node; local endpoints=''; local out
  local endpt='gluster-endpoints'; local pv; local pvc
  local mnt_perms; local mnt_gid
  local sup_gids=$USE_SGID # may have the gluster vol's gid appended

  echo
  echo "*** Gluster test suite ***"
  echo

  # get gluster vol mount perms
  node="${GLUSTER_NODES[0]}"
  echo "Connecting to $node via ssh. You may need to enter a password."
  sleep 1
  echo
  out="$(ssh $node "
          ( mnt=\$(mount |
                   grep 'fuse.glusterfs' |
                   grep $GLUSTER_VOL |
                   awk '{print \$3}') # set remote mnt var
            ls -ld \$mnt ) 2>&1
         ")"
  if (( $? == 0 )) ; then
    out=($out) # array
    mnt_perms="${out[0]}"
    mnt_gid="${out[3]}"

    # handle string gid
    if [[ ! $mnt_gid =~ ^[0-9]+$ ]] ; then # not all numeric
      # get the numeric id from the gluster storage node
      echo "Converting \"$mnt_gid\" to its numeric value on $node..."
      out="$(ssh $node "getent group $mnt_gid | cut -d: -f3")"
      if (( $? == 0 )) ; then
        mnt_gid="$out"
      else
        echo "WARN: unable to convert the \"$GLUSTER_VOL\" volume's mount's"
        echo "  string GID \"$mnt_gid\" to its numeric value. OSE doesn't"
        echo "  support string GIDs in its ID range implementation. The"
        echo "  mount's GID will be set to \"\" (nil)."
        mnt_gid=''
      fi
    fi
  else
    echo "WARN: cannot retrieve permissions for \"$GLUSTER_VOL\" from gluster"
    echo "  node $node via ssh. Be aware of potential permission mismatches"
    echo "  between openshift supplemental groups and the gluster mount."
    echo
    echo "$out"
  fi

  # append mnt's sgid to pod's sgid?
  if [[ -n "$mnt_gid" ]] ; then
    if id_in_range $mnt_gid "$SGIDs" ; then
      if (( mnt_gid != USE_SGID )) ; then # append mount gid to supp gids
        sup_gids+=",$mnt_gid"     
        echo "Pod's supplemental groups are: $sup_gids"
      fi
    else
      echo "WARN: the Gluster volume ${GLUSTER_VOL}'s GID of $mnt_gid is NOT"
      echo "  within the \"$NS\" project's range of group IDs: $SGIDs,"
      echo "  and thus cannot be added to the pod's list of group IDs."
    fi
  fi

  # show more info for running these tests...
  if (( ! QUIET )) ; then
    cat <<END_GLUSTER

    The supplied gluster storage nodes (endpoints) and the glusterfs plugin
    are tested using the busybox container to access the $GLUSTER_VOL volume.
    On one of the gluster nodes, eg. $node, ensure that gluster is
    running, the "$GLUSTER_VOL" volume is active, and the volume mount 
    has the correct permissions. Eg:
       $ gluster peer status
       $ gluster volume status $GLUSTER_VOL
       $ mount | grep glusterfs
       # if $GLUSTER_VOL is not displayed, then:
       $ mount -a # assuming the vol mount is present in /etc/fstab
       # if the vol mount is not in /etc/fstab, then add it, eg:
       ${GLUSTER_NODES[0]}:/$GLUSTER_VOL /mnt/glusterfs/$GLUSTER_VOL glusterfs _netdev 0 0
       
    The group ID for the "$GLUSTER_VOL" volume mount is:
       ${mnt_gid:-<unavailable>}
    and permissions on this volume's mount directory are:
       ${mnt_perms:-<unavailable>}

    To edit the range of supplemental group IDs, on \"$MASTER\", use:
       $ oc edit ns $NS
    and change the 'openshift.io/sa.scc.supplemental-groups' range to
    include the ${GLUSTER_VOL}'s mount's group ID.  Also, use:
       $ oc get ns $NS -o yaml
    to see the values for various IDs in the "$NS" project.

    On all of the OSE nodes make sure to:
      $ yum install glusterfs-client
      $ setsebool -P virt_sandbox_use_fusefs 1  # on, add the fusefs label
      $ setenforce 1 # keep selinux enforcing

    **NOTE: the setsebool command above. It is critical for enabling POSIX
            file access from the target containers!!

END_GLUSTER
  
    read -p "Press any key to continue..." -t120 out
  fi

  echo
  echo "----------"
  echo "Gluster Test 1: baseline: busybox, glusterfs plugin:"

  # a. create endpoints
  delete_endpoint $endpt

  for node in ${GLUSTER_NODES[@]}; do # need newlines
     endpoints+="- addresses:"$'\n'
     endpoints+="  - ip: $node"$'\n'
     endpoints+="  ports:"$'\n'
     endpoints+="  - port: 1"$'\n' # ignored
     endpoints+="    protocol: TCP"$'\n'
  done

  cat <<END | ssh $MASTER "${OC_PREFIX}oc create -f -"
apiVersion: v1
kind: Endpoints
apiVersion: v1
metadata:
  name: $endpt
subsets:
$endpoints
END

  verify_new_endpoint $endpt || {
    echo "ERROR: gluster test #1: can't create endpoints"; return 1; }

  # b. create pod
  pod='gluster-pod1'
  delete_pod $pod

  cat <<END | ssh $MASTER "${OC_PREFIX}oc create -f -"
apiVersion: v1
kind: Pod
metadata:
  name: $pod
  labels:
    name: $pod
spec:
  containers:
  - name: $pod
    image: busybox
    command: ["sleep", "60000"]
    volumeMounts:
    - name: gluster-vol1
      mountPath: /usr/share/busybox
  volumes:
  - name: gluster-vol1
    glusterfs:
      endpoints: $endpt
      path: $GLUSTER_VOL
      readOnly: false
END

  verify_new_pod $pod || {
    echo "ERROR gluster test 1: pod $pod not Running"; return 1; }

  verify_mount glusterfs $pod || {
    echo "ERROR gluster test 1: expected glusterfs mount missing"; return 1; }

  echo
  echo "----------"
  echo "Gluster Test 2: busybox, glusterfs plugin, SGID $sup_gids:"

  pod='gluster-pod2'
  delete_pod $pod
  cat <<END | ssh $MASTER "${OC_PREFIX}oc create -f -"
apiVersion: v1
kind: Pod
metadata:
  name: $pod
  labels:
    name: $pod
spec:
  containers:
  - name: $pod
    image: busybox
    command: ["sleep", "60000"]
    volumeMounts:
    - name: gluster-vol2
      mountPath: /usr/share/busybox
  securityContext:
    supplementalGroups: [$sup_gids]
    privileged: false
  volumes:
  - name: gluster-vol2
    glusterfs:
      endpoints: $endpt
      path: $GLUSTER_VOL
      readOnly: false
END

  verify_new_pod $pod || {
    echo "ERROR gluster test 2: pod $pod not Running"; return 1; }

  verify_mount glusterfs $pod || {
    echo "ERROR gluster test 2: expected glusterfs mount missing"; return 1; }

  echo
  echo "----------"
  echo "Gluster Test 3: busybox, PV, PVC, SGID $sup_gids:"

  # a. create PV
  pv='gluster-pv'
  pvc='gluster-pvc'
  delete_pvc $pvc
  delete_pv $pv

  cat <<END | ssh $MASTER "${OC_PREFIX}oc create -f -"
apiVersion: v1
kind: PersistentVolume
metadata:
  name: $pv
spec:
  capacity:
    storage: 1Gi
  accessModes:
  - ReadWriteOnce
  glusterfs:
    endpoints: $endpt
    path: $GLUSTER_VOL
    readOnly: false
END

  verify_new_pv $pv || {
    echo "ERROR: gluster test #3: can't create PV"; return 1; }

  # b. create PVC
  cat <<END | ssh $MASTER "${OC_PREFIX}oc create -f -"
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: $pvc
spec:
  accessModes:
  - ReadWriteOnce
  resources:
     requests:
       storage: 1Gi
END

  verify_new_pvc $pvc || {
    echo "ERROR: gluster test #3: can't create PVC"; return 1; }

  # c. create pod using claim
  pod='gluster-pod3'
  delete_pod $pod

  cat <<END | ssh $MASTER "${OC_PREFIX}oc create -f -"
apiVersion: v1
kind: Pod
metadata:
  name: $pod
  labels:
    name: $pod
spec:
  containers:
  - name: $pod
    image: busybox
    command: ["sleep", "60000"]
    volumeMounts:
    - name: gluster-vol3
      mountPath: /usr/share/busybox
  securityContext:
    supplementalGroups: [$sup_gids]
    privileged: false
  volumes:
  - name: gluster-vol3
    persistentVolumeClaim:
      claimName: $pvc
END

  verify_new_pod $pod || {
    echo "ERROR gluster test 3: pod $pod not Running"; return 1; }

  verify_mount glusterfs $pod || {
    echo "ERROR gluster test 3: expected glusterfs mount missing"; return 1; }

  return 0
}

# run the "ceph-rbd" tests.
function rbd_test() {

  local pod; local pv; local pvc; local dmy
  local mon; local monitors=''; local pool; local image
  local secret_name='ceph-secret'

  pool="${RBD_IMAGE%/*}" # pool name is optional
  [[ "$pool" == "$RBD_IMAGE" ]] && pool='rbd' # not specified, use default
  image="${RBD_IMAGE#*/}" # remove pool name if present

  echo
  echo "*** RBD test suite ***"
  echo

  # get ceph monitor base64 secret value if not supplied
  if [[ -z "$CEPH_SECRET64" ]] ; then
    mon="${MONITORS[0]}"; mon="${mon%:*}" # remove port if present
    echo "Calculating base64 ceph secret value..."
    echo "Connecting to $mon via ssh. You may need to enter a password."
    sleep 1
    echo

    out="$(ssh $mon "ceph auth get-key client.admin | base64")"
    if (( $? != 0 )) || [[ -z "$out" ]] ; then
      echo -e "ERROR: cannot calculate base64 secret on $mon\n$out"
      echo
      echo "     Execute: ceph auth get-key client.admin | base64"
      echo "     and re-run this script providing --ceph-secret64"
      return 1
    fi
    CEPH_SECRET64="$out"
    echo "Computed ceph secret: $CEPH_SECRET64"
    echo
  fi

  if (( ! QUIET )) ; then
    cat <<END_RBD
    Busybox is run with a volume mounted via the RBD plugin. The ceph user is
    currently hard-coded to "admin" and the file system is assumed to be ext4.
    (Both of these assumptions can easily be removed and added as script args.) 

    Ceph needs to be installed and running correctly. Here are a few tips to 
    perform on one of the monitors (eg. ${MONITORS[0]}):
      rbd create ceph-image -s 1024  # create the image
      rbd map ceph-image             # map the image to the default pool, 'rbd'
      rbd showmapped
      ls /dev/rbd*                   # see which /dev/rbd device is used
      mkfs.ext4 /dev/rbd0            # create the file system
      # provide the below output as the --ceph-secret64 value
      ceph auth get-key client.admin | base64

      # on each OSE worker node:
      yum install -y ceph-common

END_RBD
  
    read -p "Press any key to continue..." -t120 dmy
  fi

  echo
  echo "----------"
  echo "RBD Test 1: baseline: busybox, ceph-rbd plugin:"

  # a. create ceph-secret
  delete_secret $secret_name

  cat <<END | ssh $MASTER "${OC_PREFIX}oc create -f -"
apiVersion: v1
kind: Secret
metadata:
  name: $secret_name
data:
  key: $CEPH_SECRET64
END

  verify_new_secret $secret_name || {
    echo "ERROR: RBD test #1: can't create secret"; return 1; }

  # b. create monitors yaml
  for mon in ${MONITORS[@]}; do # need newlines
     monitors+="       - $mon"$'\n' # mon may include :port
  done

  # c. create pod with rbd plugin defined inline
  pod='rbd-pod1'
  delete_pod $pod

  cat <<END | ssh $MASTER "${OC_PREFIX}oc create -f -"
apiVersion: v1
kind: Pod
metadata:
  name: $pod
  labels:
    name: $pod
spec:
  containers:
  - name: $pod
    image: busybox
    command: ["sleep", "60000"]
    volumeMounts:
    - name: rbd-vol1
      mountPath: /usr/share/busybox
  securityContext:
    #fsGroup: $USE_SGID
    privileged: false
  volumes:
  - name: rbd-vol1
    rbd: 
      monitors: 
$monitors
      pool: $pool
      image: $image
      user: admin
      secretRef: 
        name: $secret_name
      fsType: ext4
      readOnly: false
END

  verify_new_pod $pod || {
    echo "ERROR rbd test 1: pod $pod not Running"; return 1; }

  verify_mount rbd $pod || {
    echo "ERROR rbd test 1: expected rbd mount missing"; return 1; }

  echo
  echo "----------"
  echo "RBD Test 2: busybox, ceph-rbd plugin, PV, PVC, SGID=$USE_SGID:"

  # a. create pv
  pv='rbd-pv'
  pvc='rbd-pvc'
  delete_pvc $pvc
  delete_pv $pv

  cat <<END | ssh $MASTER "${OC_PREFIX}oc create -f -"
apiVersion: v1
kind: PersistentVolume
metadata:
  name: $pv
spec:
  capacity:
    storage: 1Gi
  accessModes:
  - ReadWriteMany
  rbd:
    monitors: 
$monitors
    pool: $pool
    image: $image
    user: admin
    secretRef: 
      name: $secret_name
    fsType: ext4
    readOnly: false
END

  verify_new_pv $pv || {
    echo "ERROR: rbd test #2: can't create PV"; return 1; }

  # b. create PVC
  cat <<END | ssh $MASTER "${OC_PREFIX}oc create -f -"
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: $pvc
spec:
  accessModes:
  - ReadWriteMany
  resources:
     requests:
       storage: 1Gi
END

  verify_new_pvc $pvc || {
    echo "ERROR: rbd test #2: can't create PVC"; return 1; }

  # c. create pod using claim
  pod='rbd-pod2'
  delete_pod $pod

  cat <<END | ssh $MASTER "${OC_PREFIX}oc create -f -"
apiVersion: v1
kind: Pod
metadata:
  name: $pod
  labels:
    name: $pod
spec:
  containers:
  - name: $pod
    image: busybox
    command: ["sleep", "60000"]
    volumeMounts:
    - name: rbd-vol2
      mountPath: /usr/share/busybox
  securityContext:
    fsGroup: $USE_SGID
    privileged: false
  volumes:
  - name: rbd-vol2
    persistentVolumeClaim:
      claimName: $pvc
END

  verify_new_pod $pod || {
    echo "ERROR rbd test 2: pod $pod not Running"; return 1; }

  verify_mount rbd $pod || {
    echo "ERROR rbd test 2: expected rbd missing"; return 1; }

  return 0
}

# execute the requested test(s).
# $@= list of 1 or more tests (space separator)
function execute_tests() {

  local tests="$@"
  local test; local errcnt=0

  echo "*** Executing tests ..."
  echo

  # execute each requested test:
  (( GENERAL_TEST )) && {
    general_test || ((errcnt++)); }

  (( GLUSTER_TEST )) && {
    gluster_test || ((errcnt++)); }

  (( NFS_TEST )) && {
    nfs_test || ((errcnt++)); }

  (( RBD_TEST )) && {
    rbd_test || ((errcnt++)); }

  echo
  echo "***"
  echo "*** Done with tests: $errcnt errors"
  echo "***"
  echo

  (( errcnt > 0 )) && return 1
  return 0
}


## main ##

parse_cmd $@ || exit -1

NS="$(get_current_namespace)"

# set supplemental group and user IDs as globals
get_supplemental_ids $NS || exit 1

echo
if (( VERIFY_ONLY )) ; then
  echo "*** Only validating the environment on ose-master \"$MASTER\""
else
  (( NUM_TESTS > 1 )) && plural='s' || plural=''
  echo "*** Will run $NUM_TESTS test$plural on ose-master \"$MASTER\":"
  for test in ${TESTS[@]}; do
     echo "       $test"
  done
fi
echo
sleep 2

validate_ose_env || exit 1

(( ! VERIFY_ONLY )) && execute_tests ${TESTS[@]} || exit 1

exit 0
